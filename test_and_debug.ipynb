{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Completion System\n",
    "\n",
    "This is a JavaScript Code Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_file = 'trained_model_parameter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tokens from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(token_dir, is_simplify=True):\n",
    "    '''\n",
    "    load token sequence data from input path: token_dir.\n",
    "    is_simplify: whether or not simplify the value of some variable type(see function for detail)\n",
    "    return a list whose elements are lists of a token sequence\n",
    "    '''\n",
    "    token_files = [] #stored the file's path which ends with 'tokens.json' \n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            token_files.append(file_path)\n",
    "            \n",
    "   #load to a list, element is a token sequence of source code         \n",
    "    token_lists = [json.load(open(f, encoding='utf-8')) for f in token_files]\n",
    "    def simplify_token(token):\n",
    "        '''\n",
    "        Because there are too many values for type: \"Identifier\", \"String\", \"Numeric\",\n",
    "        NN may be diffcult to train because of these different value. \n",
    "        So this function can transform these types of variables to a common value\n",
    "        '''\n",
    "        if token['type'] == 'Identifier':\n",
    "            token['value'] = 'id'\n",
    "        elif token['type'] == 'Numeric':\n",
    "            token['value'] = '1'\n",
    "        elif token['type'] == 'String':\n",
    "            token['value'] = 'string'\n",
    "        else:\n",
    "            pass\n",
    "    if is_simplify:\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                simplify_token(token)\n",
    "    else:\n",
    "        pass        \n",
    "    \n",
    "    return token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building, \n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        time_begin = time.time()\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_size = len(self.tokens_set) #213\n",
    "        self.index_to_string = {i:s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s:i for i, s in enumerate(self.tokens_list)}\n",
    "        time_end =time.time()\n",
    "        print('model initialization time cost: ', time_end - time_begin)\n",
    "    \n",
    "        \n",
    "    #data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type':tokens[0], 'value':tokens[1]}\n",
    "    \n",
    "    #encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self,string):\n",
    "        vector = [0] * self.tokens_size\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "    \n",
    "    #generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        print('data processing is begining...')\n",
    "        for token_sequence in self.token_lists:#token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):#each token(type_value) in source code\n",
    "                if index > 0:\n",
    "                    token_string = self.token_to_string(token)\n",
    "                    prev_token = self.token_to_string(token_sequence[index - 1])\n",
    "                    x_data.append(self.one_hot_encoding(prev_token))\n",
    "                    y_data.append(self.one_hot_encoding(token_string))\n",
    "        print('data processing is finished..')\n",
    "        return x_data, y_data\n",
    "    \n",
    "    #neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.nn = tflearn.input_data(shape=[None, self.tokens_size])\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128, activation='')\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, self.tokens_size, activation='softmax')\n",
    "        self.nn = tflearn.regression(self.nn)\n",
    "        self.model = tflearn.DNN(self.nn)\n",
    "    \n",
    "    #load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "    \n",
    "    #training ML model\n",
    "    def train(self):\n",
    "        time_begin = time.time()\n",
    "        x_data, y_data = self.data_processing()\n",
    "        time_end = time.time()\n",
    "        print('data processing time cost: ', time_end - time_begin)\n",
    "        self.create_NN()\n",
    "        time_begin = time.time()\n",
    "        self.model.fit(x_data, y_data, n_epoch=1, batch_size=500, show_metric = True)\n",
    "        time_end = time.time()\n",
    "        print('training time cost: ', time_end - time_begin)\n",
    "        return time_end - time_begin\n",
    "        \n",
    "    #save trained model to model path\n",
    "    def save_model(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "        \n",
    "    #query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token \n",
    "        '''\n",
    "        prev_token_string = self.token_to_string(prefix[-1])\n",
    "        x = self.one_hot_encoding(prev_token_string)\n",
    "        y = self.model.predict([x])\n",
    "        predicted_seq = y[0]\n",
    "        if type(predicted_seq) is np.ndarray:\n",
    "            predicted_seq = predicted_seq.tolist()\n",
    "        best_number = predicted_seq.index(max(predicted_seq))\n",
    "        best_string = self.index_to_string[best_number]\n",
    "        best_token = self.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing is finished..\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dataset = load_tokens(train_dir)\n",
    "code_completion = Code_Completion_Model(dataset)\n",
    "use_stored_model = False\n",
    "if use_stored_model:\n",
    "    code_completion.load_model(model_file)\n",
    "else:\n",
    "    train_time = code_completion.train()\n",
    "    code_completion.save_model(model_file)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('total time cost: %.2f s, model training cost: %.2f s'%(end_time-start_time, train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hole(tokens, max_hole_size = 2):\n",
    "    '''\n",
    "    input: a tokens sequence of source code and max_hole_size\n",
    "    return: hole token to be predicted (expection)\n",
    "            token sequence before the hole(prefix)\n",
    "            token sequence after the hole(suffix)\n",
    "    '''\n",
    "    hole_size = min(random.randint(1, max_hole_size), len(tokens) - 1)\n",
    "    hole_start_index = random.randint(1, len(tokens) - hole_size)\n",
    "    hole_end_index = hole_start_index + hole_size\n",
    "    prefix = tokens[0 : hole_start_index]\n",
    "    expection = tokens[hole_start_index : hole_end_index]\n",
    "    suffix = tokens[hole_end_index : 0]\n",
    "    return prefix, expection, suffix\n",
    "\n",
    "def token_equals(token1, token2):\n",
    "    '''\n",
    "    Determining whether input two tokens are equal or not\n",
    "    '''\n",
    "    if len(token1) != len(token2):\n",
    "        return False\n",
    "    for index, t1 in enumerate(token1):\n",
    "        t2 = token2[index]\n",
    "        if t1['type'] != t2['type'] or t1['value'] != t2['value']:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query test accuracy:  0.16\n"
     ]
    }
   ],
   "source": [
    "query_test_data = load_tokens(query_dir)\n",
    "correct = 0\n",
    "correct_token_list = []\n",
    "incorrect_token_list = []\n",
    "for tokens in query_test_data:\n",
    "    prefix, expection, suffix = create_hole(tokens)\n",
    "    prediction = code_completion.query_test(prefix, suffix)\n",
    "    if token_equals(prediction, expection):\n",
    "        correct += 1\n",
    "        correct_token_list.append({'expection':expection, 'prediction':prediction})\n",
    "    else:\n",
    "        incorrect_token_list.append({'expection':expection, 'prediction':prediction})\n",
    "accuracy = correct / len(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct_token_list: \\n', correct_token_list[:5])\n",
    "print('incorrect_token_list: \\n', incorrect_token_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module\n",
    "optimization idea:\n",
    "- re-implement dnn model with tensorflow(not tflearn)\n",
    "- using embedding method rather thant one_hot_encoding\n",
    "- using a deeper and wider network\n",
    "- using LSTM\n",
    "- training model not with only one previous token, severl tokens? and following tokens?\n",
    "- try CNN\n",
    "- see each source code file as a training batch, do not combine them as a huge training data(for RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_file = 'trained_model_parameter'\n",
    "\n",
    "epoch_num = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def load_tokens(token_dir, is_simplify=True):\n",
    "    '''\n",
    "    load token sequence data from input path: token_dir.\n",
    "    is_simplify: whether or not simplify the value of some variable type(see function for detail)\n",
    "    return a list whose elements are lists of a token sequence\n",
    "    '''\n",
    "    token_files = []  # stored the file's path which ends with 'tokens.json'\n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            token_files.append(file_path)\n",
    "\n",
    "    # load to a list, element is a token sequence of source code\n",
    "    token_lists = [json.load(open(f, encoding='utf-8')) for f in token_files]\n",
    "\n",
    "    def simplify_token(token):\n",
    "        '''\n",
    "        Because there are too many values for type: \"Identifier\", \"String\", \"Numeric\",\n",
    "        NN may be diffcult to train because of these different value.\n",
    "        So this function can transform these types of variables to a common value\n",
    "        '''\n",
    "        if token['type'] == 'Identifier':\n",
    "            token['value'] = 'id'\n",
    "        elif token['type'] == 'Numeric':\n",
    "            token['value'] = '1'\n",
    "        elif token['type'] == 'String':\n",
    "            token['value'] = 'string'\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if is_simplify:\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                simplify_token(token)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return token_lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building,\n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        time_begin = time.time()\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_size = len(self.tokens_set)  # 213\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_list)}\n",
    "        time_end = time.time()\n",
    "        print('model initialization time cost: ', time_end - time_begin)\n",
    "\n",
    "    # data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type': tokens[0], 'value': tokens[1]}\n",
    "\n",
    "    # encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self, string):\n",
    "        vector = [0] * self.tokens_size\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        print('data processing is begining...')\n",
    "        for token_sequence in self.token_lists:  # token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):  # each token(type_value) in source code\n",
    "                if index > 0:\n",
    "                    token_string = self.token_to_string(token)\n",
    "                    prev_token = self.token_to_string(token_sequence[index - 1])\n",
    "                    x_data.append(self.one_hot_encoding(prev_token))\n",
    "                    y_data.append(self.one_hot_encoding(token_string))\n",
    "        print('data processing is finished..')\n",
    "        pickle.dump((x_data, y_data), open('processed_data/saved_data_for_basic.p', 'wb'))\n",
    "        return x_data, y_data\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "#         self.input_x = tf.layers.Input(shape=[self.tokens_size])\n",
    "#         self.output_y = tf.layers.Input(shape=[self.tokens_size])\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size])\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size])\n",
    "        self.nn = tf.layers.dense(inputs=self.input_x, units=128, activation=tf.nn.relu)\n",
    "        self.output = tf.layers.dense(inputs=self.nn, units=self.tokens_size, activation=None)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.output_y)\n",
    "        self.loss = tf.reduce_sum(self.loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n",
    "        self.equal = tf.equal(tf.argmax(self.output_y,1), tf.argmax(self.output, 1))\n",
    "        self.accuarcy = tf.reduce_mean(tf.cast(self.equal, tf.float32))\n",
    "\n",
    "\n",
    "    # training ML model\n",
    "    def train(self, use_saved_data=False):\n",
    "        time_begin = time.time()\n",
    "        if use_saved_data:\n",
    "            x_data, y_data = pickle.load(open('processed_data/saved_data_for_basic.p', 'rb'))\n",
    "        else:\n",
    "            x_data, y_data = self.data_processing()\n",
    "            \n",
    "        time_end = time.time()\n",
    "        print('data processing time cost: ', time_end - time_begin)\n",
    "        self.create_NN()\n",
    "        time_begin = time.time()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(epoch_num):\n",
    "                for i in range(0, len(x_data), batch_size):\n",
    "                    batch_x = x_data[i:i+batch_size]\n",
    "                    batch_y = y_data[i:i+batch_size]\n",
    "                    feed = {self.input_x:batch_x, self.output_y:batch_y}\n",
    "                    sess.run(self.optimizer, feed_dict=feed)\n",
    "                    if (i//batch_size) % 500 == 0:\n",
    "                        show_acc = sess.run(self.accuarcy, feed_dict=feed)\n",
    "                        print('epoch: %d, training_step: %d, accuracy:%.3f'%(epoch, i, show_acc))\n",
    "\n",
    "        time_end = time.time()\n",
    "        print('training time cost: ', time_end - time_begin)\n",
    "        return time_end - time_begin\n",
    "\n",
    "\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = self.token_to_string(prefix[-1])\n",
    "        x = self.one_hot_encoding(prev_token_string)\n",
    "        with tf.Session() as sess:\n",
    "            feed = {self.input_x:x}\n",
    "            predict_list = sess.run(self.output, feed_dict=feed)\n",
    "            prediction = tf.argmax(predict_list, 1)\n",
    "            best_string = self.index_to_string[prediction]\n",
    "            best_token = self.string_to_token(best_string)\n",
    "        return [best_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_tokens(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialization time cost:  0.8267886638641357\n",
      "data processing is begining...\n",
      "data processing is finished..\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4d9b12cb70f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muse_stored_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_completion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-f9ac002433d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, use_saved_data)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data/saved_data_for_basic.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-f9ac002433d5>\u001b[0m in \u001b[0;36mdata_processing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data processing is finished..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data/saved_data_for_basic.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "code_completion = Code_Completion_Model(dataset)\n",
    "use_stored_model = False\n",
    "\n",
    "train_time = code_completion.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "\n",
    "\n",
    "import data_utils\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_dir = 'saved_model/model_parameter'\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building,\n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.string_to_index, self.index_to_string, token_set = \\\n",
    "            data_utils.load_data_with_pickle('processed_data/train_parameter.p')\n",
    "        self.num_token = len(token_set)\n",
    "\n",
    "        \n",
    "    def init_with_orig_data(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        self.dataset = token_lists\n",
    "        self.tokens_set = data_utils.get_token_set(self.dataset)\n",
    "        self.num_tokens = len(self.tokens_set)  # 74 经过简化后只有74种token\n",
    "        print(self.num_tokens)\n",
    "        # 构建映射字典\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_set)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_set)}\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for index, token in enumerate(self.dataset):\n",
    "            if index > 0:\n",
    "                token_string = data_utils.token_to_string(token)\n",
    "                prev_token = data_utils.token_to_string(self.dataset[index - 1])\n",
    "                x_data.append(self.one_hot_encoding(prev_token))\n",
    "                y_data.append(self.one_hot_encoding(token_string))\n",
    "        return x_data, y_data\n",
    "\n",
    "    def vector_data_process(self,dataset):\n",
    "        '''\n",
    "        读取已经被处理成one_hot_vector的token data，该函数会根据该dataset\n",
    "        构造x_data and y_data\n",
    "        :param dataset:\n",
    "        :return:\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for index, token in enumerate(dataset):\n",
    "            if index > 0:\n",
    "                x_data.append(dataset[index])\n",
    "                y_data.append(token)\n",
    "        return x_data, y_data\n",
    "\n",
    "\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.nn = tflearn.input_data(shape=[None, self.num_token])\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, self.num_token, activation='softmax')\n",
    "        self.nn = tflearn.regression(self.nn)\n",
    "        self.model = tflearn.DNN(self.nn)\n",
    "\n",
    "    # load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "\n",
    "    # training ML model\n",
    "    def train(self, train_data, with_original_data=False):\n",
    "        print('model training...')\n",
    "        if with_original_data:\n",
    "            self.init_with_orig_data(train_data)\n",
    "            x_data, y_data = self.data_processing()\n",
    "            self.create_NN()\n",
    "            self.model.fit(x_data, y_data, n_epoch=1, batch_size=500, show_metric=True)\n",
    "        else:\n",
    "            x_data, y_data = self.vector_data_process(train_data)\n",
    "            self.create_NN()\n",
    "            self.model.fit(\n",
    "                x_data, y_data, n_epoch=1, validation_set=0.2, batch_size=500, show_metric=True)\n",
    "\n",
    "    # save trained model to model path\n",
    "    def save_model(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = data_utils.token_to_string(prefix[-1])\n",
    "        x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "        y = self.model.predict([x])\n",
    "        predicted_seq = y[0]\n",
    "        if type(predicted_seq) is np.ndarray:\n",
    "            predicted_seq = predicted_seq.tolist()\n",
    "        best_number = predicted_seq.index(max(predicted_seq))\n",
    "        print('prediction:', best_number)\n",
    "        best_string = self.index_to_string[best_number]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for tokens in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "            prediction = self.query_test(prefix, suffix)\n",
    "\n",
    "            if data_utils.token_equals(prediction, expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_utils.load_data_with_pickle('processed_data/vec_train_data.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load and model create\n",
    "cc_model = Code_Completion_Model()\n",
    "#training model\n",
    "use_stored_model = False\n",
    "if use_stored_model:\n",
    "    cc_model.load_model(model_dir)\n",
    "else:\n",
    "    cc_model.train(processed_data, with_original_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_data = data_utils.load_data_with_file(query_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_test_haha(query_test_data):\n",
    "    '''\n",
    "    Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "    ML model will predict the most probable token in the hole\n",
    "    In this function, use only one token before hole token to predict\n",
    "    return: the most probable token\n",
    "    '''\n",
    "    correct = 0\n",
    "    correct_token_list = []\n",
    "    incorrect_token_list = []\n",
    "    for tokens in query_test_data:\n",
    "        prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "        prediction = cc_model.query_test(prefix, suffix)\n",
    "        \n",
    "        strring = data_utils.token_to_string(expection[0])\n",
    "        #print(strring)\n",
    "        index_num = cc_model.string_to_index[strring]\n",
    "        print('expection:', index_num)\n",
    "        print('\\n')\n",
    "        if data_utils.token_equals(prediction, expection):\n",
    "            correct += 1\n",
    "            correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        else:\n",
    "            incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "    accuracy = correct / len(query_test_data)\n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "使用TensorFlow自带的layers构建基本的神经网络对token进行预测，\n",
    "可以声明使用多少个context tokens 进行预测\n",
    "\n",
    "多个previous token输入神经网络的方法有两种想法：\n",
    "1. 将每个token的representation vector相连，合成一个大的vector输入到神经网络，\n",
    "    所以说神经网络的输入层大小应为：每个token vector length * number of previous token\n",
    "2. 应为目前表示每个token 使用的方法为one hot encoding，也就是说对每个token都是有且仅有一位为1，其余位为0，\n",
    "    所以可以考虑直接将所有的previous token相加，这样做的好处是NN输入层大小永远等于vector length。缺点是没有理论依据，不知道效果是否会更好\n",
    "\n",
    "\n",
    "1. concatenate the representations of previous tokens to a huge vector representation\n",
    "2. add the representations of previous tokens together\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "\n",
    "epoch_num = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "previous_token_num = 2\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "\n",
    "    def __init__(self, x_data, y_data, token_set, string2int, int2string):\n",
    "        batch_num = len(x_data) // batch_size\n",
    "        self.x_data = x_data[:batch_num * batch_size]\n",
    "        self.y_data = y_data[:batch_num * batch_size]\n",
    "        self.index_to_string = int2string\n",
    "        self.string_to_index = string2int\n",
    "        self.tokens_set = token_set\n",
    "        self.tokens_size = len(token_set)\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='input_x')\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='output_y')\n",
    "        self.nn = tf.layers.dense(inputs=self.input_x, units=128, activation=tf.nn.relu, name='hidden_1')\n",
    "        self.output = tf.layers.dense(inputs=self.nn, units=self.tokens_size, activation=None, name='prediction')\n",
    "        self.prediction_index = tf.argmax(self.output, 1)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.output_y, name='loss')\n",
    "        self.loss = tf.reduce_sum(self.loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n",
    "        self.equal = tf.equal(tf.argmax(self.output_y, 1), tf.argmax(self.output, 1))\n",
    "        self.accuarcy = tf.reduce_mean(tf.cast(self.equal, tf.float32), name='accuracy')\n",
    "\n",
    "    def get_batch(self, context_size = previous_token_num):\n",
    "        \n",
    "        x_data = np.array(self.x_data)\n",
    "        for i in range(0, len(self.x_data), batch_size):\n",
    "            batch_x = np.zeros((batch_size, self.tokens_size))\n",
    "            for j in range(context_size):\n",
    "                if i >= j:\n",
    "                    temp = x_data[i-j:i-j+batch_size].reshape(-1, self.tokens_size)\n",
    "                    if temp.shape == (0, 86): break;\n",
    "                    batch_x += temp\n",
    "            batch_y = self.y_data[i:i + batch_size]\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def train(self):\n",
    "        self.create_NN()\n",
    "        self.sess = tf.Session()\n",
    "        time_begin = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        batch_generator = self.get_batch()\n",
    "        for epoch in range(epoch_num):\n",
    "            for i in range(0, len(self.x_data), batch_size):\n",
    "                batch_x, batch_y = next(batch_generator)\n",
    "                feed = {self.input_x: batch_x, self.output_y: batch_y}\n",
    "                self.sess.run(self.optimizer, feed_dict=feed)\n",
    "                if (i // batch_size) % 2000 == 0:\n",
    "                    show_loss, show_acc = self.sess.run([self.loss, self.accuarcy], feed_dict=feed)\n",
    "                    print('epoch: %d, training_step: %d, loss: %.2f, accuracy:%.3f' % (epoch, i, show_loss, show_acc))\n",
    "        time_end = time.time()\n",
    "        print('training time cost: %.3f s'%(time_end - time_begin))\n",
    "\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole. In this function, use only one token before hole token to predict\n",
    "        '''\n",
    "        previous_token_list = prefix[-previous_token_num:]\n",
    "        context_representation = np.zeros(self.tokens_size)\n",
    "\n",
    "        for token in previous_token_list:\n",
    "            prev_token_string = data_utils.token_to_string(token)\n",
    "            pre_token_x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "            context_representation += np.array(pre_token_x)\n",
    "\n",
    "        feed = {self.input_x: [context_representation]}\n",
    "        prediction = self.sess.run(self.prediction_index, feed)[0]\n",
    "        best_string = self.index_to_string[prediction]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    #test model\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prediction = self.query_test(prefix, suffix)[0]\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "x_data = data_utils.load_data_with_pickle(x_train_data_path)\n",
    "y_data = data_utils.load_data_with_pickle(y_train_data_path)\n",
    "token_set, string2int, int2string = data_utils.load_data_with_pickle(train_data_parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training_step: 0, loss: 276.29, accuracy:0.172\n",
      "epoch: 0, training_step: 128000, loss: 89.34, accuracy:0.562\n",
      "epoch: 0, training_step: 256000, loss: 82.85, accuracy:0.531\n",
      "epoch: 0, training_step: 384000, loss: 91.51, accuracy:0.547\n",
      "epoch: 0, training_step: 512000, loss: 110.68, accuracy:0.391\n",
      "epoch: 0, training_step: 640000, loss: 97.37, accuracy:0.469\n",
      "epoch: 0, training_step: 768000, loss: 35.40, accuracy:0.719\n",
      "epoch: 0, training_step: 896000, loss: 127.04, accuracy:0.500\n",
      "epoch: 0, training_step: 1024000, loss: 57.99, accuracy:0.625\n",
      "epoch: 0, training_step: 1152000, loss: 66.93, accuracy:0.672\n",
      "epoch: 0, training_step: 1280000, loss: 128.33, accuracy:0.344\n",
      "epoch: 0, training_step: 1408000, loss: 98.13, accuracy:0.484\n",
      "epoch: 0, training_step: 1536000, loss: 130.28, accuracy:0.391\n",
      "epoch: 0, training_step: 1664000, loss: 108.26, accuracy:0.391\n",
      "training time cost:  51.472346782684326\n"
     ]
    }
   ],
   "source": [
    "#model train\n",
    "model = Code_Completion_Model(x_data, y_data, token_set, string2int, int2string)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "query_test_data = data_utils.load_data_with_file(query_dir)\n",
    "accuracy = model.test_model(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(12).reshape(6,2)\n",
    "aa = np.array([-2,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data += aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "if aa.shape == (2,):\n",
    "    print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2, -1],\n",
       "       [ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]]\n",
      "[[2. 4.]\n",
      " [6. 8.]]\n",
      "[[ 6.  8.]\n",
      " [10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    bb = 2\n",
    "    batch = np.zeros((2, 2))\n",
    "    for j in range(2):\n",
    "        if (i>=j):\n",
    "            temp = data[i-j:i+bb-j]\n",
    "            batch += temp\n",
    "    print(batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
