{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Completion System\n",
    "\n",
    "This is a JavaScript Code Prediction System（with Huge dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_example = [{'id': 0, 'type': 'Program', 'children': [1]},\n",
    "                   {'id': 1, 'type': 'ExpressionStatement', 'children': [2]},\n",
    "                   {'id': 2, 'type': 'CallExpression', 'children': [3, 8, 9, 10]},\n",
    "                   {'id': 3, 'type': 'MemberExpression', 'children': [4, 7]},\n",
    "                   {'id': 4, 'type': 'MemberExpression', 'children': [5, 6]},\n",
    "                   {'id': 5, 'type': 'Identifier', 'value': 'CKEDITOR'},\n",
    "                   {'id': 6, 'type': 'Property', 'value': 'plugins'},\n",
    "                   {'id': 7, 'type': 'Property', 'value': 'setLang'},\n",
    "                   {'id': 8, 'type': 'LiteralString', 'value': 'iframe'},\n",
    "                   {'id': 9, 'type': 'LiteralString', 'value': 'ka'},\n",
    "                   {'id': 10, 'type': 'ObjectExpression', 'children': [11, 13, 15, 17, 19]},\n",
    "                   {'id': 11, 'type': 'Property', 'value': 'border', 'children': [12]},\n",
    "                   {'id': 12, 'type': 'LiteralString', 'value': 'ჩარჩოს გამოჩენა'},\n",
    "                   {'id': 13, 'type': 'Property', 'value': 'noUrl', 'children': [14]},\n",
    "                   {'id': 14, 'type': 'LiteralString', 'value': 'აკრიფეთ iframe-ის URL'},\n",
    "                   {'id': 15, 'type': 'Property', 'value': 'scrolling', 'children': [16]},\n",
    "                   {'id': 16, 'type': 'LiteralString', 'value': 'გადახვევის ზოლების დაშვება'},\n",
    "                   {'id': 17, 'type': 'Property', 'value': 'title', 'children': [18]},\n",
    "                   {'id': 18, 'type': 'LiteralString', 'value': 'IFrame-ის პარამეტრები'},\n",
    "                   {'id': 19, 'type': 'Property', 'value': 'toolbar', 'children': [20]},\n",
    "                   {'id': 20, 'type': 'LiteralString', 'value': 'IFrame'}, 0]\n",
    "\n",
    "\n",
    "def bulid_binary_tree(ast):\n",
    "    \"\"\"transform the AST(one node may have several childNodes) to\n",
    "    Left-Child-Right-Sibling(LCRS) binary tree.\"\"\"\n",
    "    brother_map = {0: -1}\n",
    "    for index, node in enumerate(ast):  # 顺序遍历每个AST中的node\n",
    "\n",
    "        if not isinstance(node, dict) and node == 0:  # AST中最后添加一个'EOF’标识\n",
    "            ast[index] = 'EOF'\n",
    "            break  # return data\n",
    "\n",
    "        node['right'] = brother_map.get(node['id'], -1)\n",
    "\n",
    "        if 'children' in node.keys():  # 表示该node为non-terminal\n",
    "            node['isTerminal'] = False\n",
    "            add_two_bits_info(ast, node, brother_map)  # 向每个节点添加两bit的额外信息\n",
    "            child_list = node['children']\n",
    "            node['left'] = child_list[0]  # 构建该node的left node\n",
    "            for i, bro in enumerate(child_list):  # 为该node的所有children构建right sibling\n",
    "                if i == len(child_list) - 1:\n",
    "                    break\n",
    "                brother_map[bro] = child_list[i + 1]\n",
    "        else:\n",
    "            node['isTerminal'] = True\n",
    "    return ast\n",
    "\n",
    "\n",
    "def add_two_bits_info(ast, node, brother_map):\n",
    "    # 向每个节点添加两bit的额外信息：hasNonTerminalChild和hasSibling\n",
    "    node['hasNonTerminalChild'] = False\n",
    "    for child_index in node['children']:\n",
    "        if 'children' in ast[child_index]:\n",
    "            node['hasNonTerminalChild'] = True\n",
    "            break\n",
    "    if brother_map.get(node['id'], -1) == -1:\n",
    "        node['hasSibling'] = False\n",
    "    else:\n",
    "        node['hasSibling'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30001\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "tt_token_to_int, tt_int_to_token, nt_token_to_int, nt_int_to_token = utils.load_dict_parameter()\n",
    "print(len(tt_token_to_int))\n",
    "print(len(nt_token_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'type': 'Program',\n",
       "  'children': [1],\n",
       "  'right': -1,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': True,\n",
       "  'hasSibling': False,\n",
       "  'left': 1},\n",
       " {'id': 1,\n",
       "  'type': 'ExpressionStatement',\n",
       "  'children': [2],\n",
       "  'right': -1,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': True,\n",
       "  'hasSibling': False,\n",
       "  'left': 2},\n",
       " {'id': 2,\n",
       "  'type': 'CallExpression',\n",
       "  'children': [3, 8, 9, 10],\n",
       "  'right': -1,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': True,\n",
       "  'hasSibling': False,\n",
       "  'left': 3},\n",
       " {'id': 3,\n",
       "  'type': 'MemberExpression',\n",
       "  'children': [4, 7],\n",
       "  'right': 8,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': True,\n",
       "  'hasSibling': True,\n",
       "  'left': 4},\n",
       " {'id': 4,\n",
       "  'type': 'MemberExpression',\n",
       "  'children': [5, 6],\n",
       "  'right': 7,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': False,\n",
       "  'hasSibling': True,\n",
       "  'left': 5},\n",
       " {'id': 5,\n",
       "  'type': 'Identifier',\n",
       "  'value': 'CKEDITOR',\n",
       "  'right': 6,\n",
       "  'isTerminal': True},\n",
       " {'id': 6,\n",
       "  'type': 'Property',\n",
       "  'value': 'plugins',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " {'id': 7,\n",
       "  'type': 'Property',\n",
       "  'value': 'setLang',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " {'id': 8,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'iframe',\n",
       "  'right': 9,\n",
       "  'isTerminal': True},\n",
       " {'id': 9,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'ka',\n",
       "  'right': 10,\n",
       "  'isTerminal': True},\n",
       " {'id': 10,\n",
       "  'type': 'ObjectExpression',\n",
       "  'children': [11, 13, 15, 17, 19],\n",
       "  'right': -1,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': True,\n",
       "  'hasSibling': False,\n",
       "  'left': 11},\n",
       " {'id': 11,\n",
       "  'type': 'Property',\n",
       "  'value': 'border',\n",
       "  'children': [12],\n",
       "  'right': 13,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': False,\n",
       "  'hasSibling': True,\n",
       "  'left': 12},\n",
       " {'id': 12,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'ჩარჩოს გამოჩენა',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " {'id': 13,\n",
       "  'type': 'Property',\n",
       "  'value': 'noUrl',\n",
       "  'children': [14],\n",
       "  'right': 15,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': False,\n",
       "  'hasSibling': True,\n",
       "  'left': 14},\n",
       " {'id': 14,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'აკრიფეთ iframe-ის URL',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " {'id': 15,\n",
       "  'type': 'Property',\n",
       "  'value': 'scrolling',\n",
       "  'children': [16],\n",
       "  'right': 17,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': False,\n",
       "  'hasSibling': True,\n",
       "  'left': 16},\n",
       " {'id': 16,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'გადახვევის ზოლების დაშვება',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " {'id': 17,\n",
       "  'type': 'Property',\n",
       "  'value': 'title',\n",
       "  'children': [18],\n",
       "  'right': 19,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': False,\n",
       "  'hasSibling': True,\n",
       "  'left': 18},\n",
       " {'id': 18,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'IFrame-ის პარამეტრები',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " {'id': 19,\n",
       "  'type': 'Property',\n",
       "  'value': 'toolbar',\n",
       "  'children': [20],\n",
       "  'right': -1,\n",
       "  'isTerminal': False,\n",
       "  'hasNonTerminalChild': False,\n",
       "  'hasSibling': False,\n",
       "  'left': 20},\n",
       " {'id': 20,\n",
       "  'type': 'LiteralString',\n",
       "  'value': 'IFrame',\n",
       "  'right': -1,\n",
       "  'isTerminal': True},\n",
       " 'EOF']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_tree = bulid_binary_tree(ast_example)\n",
    "bin_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'type': 'Program', 'children': [1], 'right': -1, 'isTerminal': False, 'hasNonTerminalChild': True, 'hasSibling': False, 'left': 1}, {'id': 1, 'type': 'ExpressionStatement', 'children': [2], 'right': -1, 'isTerminal': False, 'hasNonTerminalChild': True, 'hasSibling': False, 'left': 2}, {'id': 2, 'type': 'CallExpression', 'children': [3, 8, 9, 10], 'right': -1, 'isTerminal': False, 'hasNonTerminalChild': True, 'hasSibling': False, 'left': 3}, {'id': 3, 'type': 'MemberExpression', 'children': [4, 7], 'right': 8, 'isTerminal': False, 'hasNonTerminalChild': True, 'hasSibling': True, 'left': 4}, {'id': 4, 'type': 'MemberExpression', 'children': [5, 6], 'right': 7, 'isTerminal': False, 'hasNonTerminalChild': False, 'hasSibling': True, 'left': 5}, {'id': 5, 'type': 'Identifier', 'value': 'CKEDITOR', 'right': 6, 'isTerminal': True}, {'id': 6, 'type': 'Property', 'value': 'plugins', 'right': -1, 'isTerminal': True}, {'id': 7, 'type': 'Property', 'value': 'setLang', 'right': -1, 'isTerminal': True}, {'id': 8, 'type': 'LiteralString', 'value': 'iframe', 'right': 9, 'isTerminal': True}, {'id': 9, 'type': 'LiteralString', 'value': 'ka', 'right': 10, 'isTerminal': True}, {'id': 10, 'type': 'ObjectExpression', 'children': [11, 13, 15, 17, 19], 'right': -1, 'isTerminal': False, 'hasNonTerminalChild': True, 'hasSibling': False, 'left': 11}, {'id': 11, 'type': 'Property', 'value': 'border', 'children': [12], 'right': 13, 'isTerminal': False, 'hasNonTerminalChild': False, 'hasSibling': True, 'left': 12}, {'id': 12, 'type': 'LiteralString', 'value': 'ჩარჩოს გამოჩენა', 'right': -1, 'isTerminal': True}, {'id': 13, 'type': 'Property', 'value': 'noUrl', 'children': [14], 'right': 15, 'isTerminal': False, 'hasNonTerminalChild': False, 'hasSibling': True, 'left': 14}, {'id': 14, 'type': 'LiteralString', 'value': 'აკრიფეთ iframe-ის URL', 'right': -1, 'isTerminal': True}, {'id': 15, 'type': 'Property', 'value': 'scrolling', 'children': [16], 'right': 17, 'isTerminal': False, 'hasNonTerminalChild': False, 'hasSibling': True, 'left': 16}, {'id': 16, 'type': 'LiteralString', 'value': 'გადახვევის ზოლების დაშვება', 'right': -1, 'isTerminal': True}, {'id': 17, 'type': 'Property', 'value': 'title', 'children': [18], 'right': 19, 'isTerminal': False, 'hasNonTerminalChild': False, 'hasSibling': True, 'left': 18}, {'id': 18, 'type': 'LiteralString', 'value': 'IFrame-ის პარამეტრები', 'right': -1, 'isTerminal': True}, {'id': 19, 'type': 'Property', 'value': 'toolbar', 'children': [20], 'right': -1, 'isTerminal': False, 'hasNonTerminalChild': False, 'hasSibling': False, 'left': 20}, {'id': 20, 'type': 'LiteralString', 'value': 'IFrame', 'right': -1, 'isTerminal': True}, 'EOF']\n"
     ]
    }
   ],
   "source": [
    "print(bin_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ast_to_seq(binary_tree):\n",
    "    # 将一个ast首先转换成二叉树，然后对该二叉树进行中序遍历，得到nt_sequence\n",
    "    def node_to_string(node):\n",
    "        # 将一个node转换为string\n",
    "        if node == 'EMPTY':\n",
    "            string_node = 'EMPTY'\n",
    "        elif node['isTerminal']:  # 如果node为terminal\n",
    "            string_node = str(node['type']) + '=$$=' + \\\n",
    "                str(node['value'])   + '==' + str(node['id'])\n",
    "            terminal_count[string_node] += 1\n",
    "        else:  # 如果是non-terminal\n",
    "            string_node = str(node['type']) + '=$$=' + \\\n",
    "                str(node['hasSibling']) + '=$$=' + \\\n",
    "                str(node['hasNonTerminalChild'])  + str(node['id'])\n",
    "                # 有些non-terminal包含value，探索该value的意义？（value种类非常多）\n",
    "            non_termial_set.add(string_node)\n",
    "        return string_node\n",
    "\n",
    "    def in_order_traversal(bin_tree, index):\n",
    "        # 对给定的二叉树进行中序遍历，并在中序遍历的时候，生成nt_pair\n",
    "        node = bin_tree[index]\n",
    "        if 'left' in node.keys():\n",
    "            in_order_traversal(bin_tree, node['left'])\n",
    "\n",
    "        if 'isTerminal' in node.keys() and node['isTerminal'] is False:\n",
    "            # 如果该node是non-terminal，并且包含一个terminal 子节点，则和该子节点组成nt_pair保存在output中\n",
    "            # 否则将nt_pair的T设为字符串EMPTY\n",
    "            n_pair = node_to_string(node)\n",
    "            for child_index in node['children']:  # 遍历该Nterminal的所有child，分别用所有child构建NT-pair\n",
    "                if bin_tree[child_index]['isTerminal']:\n",
    "                    t_pair = node_to_string(bin_tree[child_index])\n",
    "                else:\n",
    "                    t_pair = node_to_string('EMPTY')\n",
    "                nt_pair = (n_pair, t_pair)\n",
    "                output.append(nt_pair)\n",
    "\n",
    "            # if bin_tree[node['left']]['isTerminal']:\n",
    "            #     assert bin_tree[node['left']]['id'] == node['left']\n",
    "            #     t_pair = node_to_string(bin_tree[node['left']])\n",
    "            # else:\n",
    "            #     t_pair = node_to_string('EMPTY')\n",
    "            # nt_pair = (n_pair, t_pair)\n",
    "            # output.append(nt_pair)\n",
    "        else:  # 该token是terminal，只将其记录到counter中\n",
    "            node_to_string(node)\n",
    "\n",
    "        if node['right'] != -1:  # 遍历right side\n",
    "            in_order_traversal(bin_tree, node['right'])\n",
    "\n",
    "    output = []\n",
    "    in_order_traversal(binary_tree, 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "terminal_count = Counter()  # 统计每个terminal token的出现次数\n",
    "non_termial_set = set()  # 统计non_termial token 种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MemberExpression=$$=True=$$=False4', 'Identifier=$$=CKEDITOR==5'),\n",
       " ('MemberExpression=$$=True=$$=False4', 'Property=$$=plugins==6'),\n",
       " ('MemberExpression=$$=True=$$=True3', 'EMPTY'),\n",
       " ('MemberExpression=$$=True=$$=True3', 'Property=$$=setLang==7'),\n",
       " ('Property=$$=True=$$=False11', 'LiteralString=$$=ჩარჩოს გამოჩენა==12'),\n",
       " ('Property=$$=True=$$=False13', 'LiteralString=$$=აკრიფეთ iframe-ის URL==14'),\n",
       " ('Property=$$=True=$$=False15',\n",
       "  'LiteralString=$$=გადახვევის ზოლების დაშვება==16'),\n",
       " ('Property=$$=True=$$=False17', 'LiteralString=$$=IFrame-ის პარამეტრები==18'),\n",
       " ('Property=$$=False=$$=False19', 'LiteralString=$$=IFrame==20'),\n",
       " ('ObjectExpression=$$=False=$$=True10', 'EMPTY'),\n",
       " ('ObjectExpression=$$=False=$$=True10', 'EMPTY'),\n",
       " ('ObjectExpression=$$=False=$$=True10', 'EMPTY'),\n",
       " ('ObjectExpression=$$=False=$$=True10', 'EMPTY'),\n",
       " ('ObjectExpression=$$=False=$$=True10', 'EMPTY'),\n",
       " ('CallExpression=$$=False=$$=True2', 'EMPTY'),\n",
       " ('CallExpression=$$=False=$$=True2', 'LiteralString=$$=iframe==8'),\n",
       " ('CallExpression=$$=False=$$=True2', 'LiteralString=$$=ka==9'),\n",
       " ('CallExpression=$$=False=$$=True2', 'EMPTY'),\n",
       " ('ExpressionStatement=$$=False=$$=True1', 'EMPTY'),\n",
       " ('Program=$$=False=$$=True0', 'EMPTY')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = ast_to_seq(bin_tree)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "js_test_data_dir = 'js_dataset/js_programs_eval.json'\n",
    "js_train_data_dir = 'js_dataset/js_programs_training.json'\n",
    "\n",
    "def dataset_split(is_training=True, subset_size=5000):\n",
    "    if is_training:\n",
    "        data_path = js_train_data_dir\n",
    "        total_size = 100000\n",
    "        saved_to_path = 'split_js_data/train_data/'\n",
    "    else:\n",
    "        data_path = js_test_data_dir\n",
    "        total_size = 50000\n",
    "        saved_to_path = 'split_js_data/eval_data/'\n",
    "\n",
    "    file = open(data_path, 'r')\n",
    "    subset_list = []\n",
    "    error_count = 0\n",
    "    for i in range(1, total_size + 1):\n",
    "        try:\n",
    "            line = file.readline()\n",
    "            line = json.loads(line)\n",
    "            nt_seq = ast_to_seq(line)\n",
    "        except:\n",
    "            error_count += 1\n",
    "           # print('UTF-8 error: {}/{}'.format(error_count, i))\n",
    "        subset_list.append(nt_seq)\n",
    "        if i % subset_size == 0:\n",
    "#             sub_path = saved_to_path + 'part{}'.format(i // subset_size) + '.json'\n",
    "#             save_file = open(sub_path, 'wb')\n",
    "#             pickle.dump(subset_list, save_file)\n",
    "            subset_list = []\n",
    "    print('data seperating finished..., utf-8 error:{}'.format(error_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_two_bits_info(node, brother_map):\n",
    "    # 向每个节点添加两bit的额外信息：isTerminal和hasSibling\n",
    "    if 'children' in node.keys():\n",
    "        node['isTerminal'] = False\n",
    "    else:\n",
    "        node['isTerminal'] = True\n",
    "    if brother_map.get(node['id'], -1) == -1:\n",
    "        node['hasSibling'] = False\n",
    "    else:\n",
    "        node['hasSibling'] = True\n",
    "\n",
    "\n",
    "def bulid_binary_tree(data):\n",
    "    '''\n",
    "    transform the AST(one node may have several childNodes) to \n",
    "    Left-Child-Right-Sibling(LCRS) binary tree.\n",
    "    '''\n",
    "    brother_map = {0: -1}\n",
    "    for index, node in enumerate(data): # 顺序遍历每个AST中的node\n",
    "        if type(node) != dict and node == 0: # AST中最后添加一个'EOF’标识\n",
    "            data[index] = 'EOF'\n",
    "            return data\n",
    "        # 向每个节点添加两bit的额外信息\n",
    "        add_two_bits_info(node, brother_map)\n",
    "        node['right'] = brother_map.get(node['id'], -1)\n",
    "        # 表示该node为non-terminal\n",
    "        if 'children' in node.keys():\n",
    "            child_list = node['children']\n",
    "            node['left'] = child_list[0] # 构建该node的left node\n",
    "            for i, bro in enumerate(child_list): # 为该node的所有children构建right sibling\n",
    "                if i == len(child_list)-1:\n",
    "                    break\n",
    "                brother_map[bro] = child_list[i+1]\n",
    "            node.pop('children')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MemberExpression=$$=True=$$=False', 'Identifier=$$=CKEDITOR'), ('MemberExpression=$$=True=$$=False', 'EMPTY'), ('Property=$$=True=$$=False=$$=border', 'LiteralString=$$=ჩარჩოს გამოჩენა'), ('Property=$$=True=$$=False=$$=noUrl', 'LiteralString=$$=აკრიფეთ iframe-ის URL'), ('Property=$$=True=$$=False=$$=scrolling', 'LiteralString=$$=გადახვევის ზოლების დაშვება'), ('Property=$$=True=$$=False=$$=title', 'LiteralString=$$=IFrame-ის პარამეტრები'), ('Property=$$=False=$$=False=$$=toolbar', 'LiteralString=$$=IFrame'), ('ObjectExpression=$$=False=$$=False', 'EMPTY'), ('CallExpression=$$=False=$$=False', 'EMPTY'), ('ExpressionStatement=$$=False=$$=False', 'EMPTY'), ('Program=$$=False=$$=False', 'EMPTY')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token2int = {}\n",
    "int2token = {}\n",
    "terminalCountMap = Counter()\n",
    "nonTerminalSet =  set()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def ast_to_seq(data):\n",
    "    bi_tree = bulid_binary_tree(data)\n",
    " #   print(bi_tree)\n",
    "    \n",
    "    def node_to_string(node):\n",
    "        if node == 'EMPTY':\n",
    "            string_node = 'EMPTY'\n",
    "        elif node['isTerminal']:  # 如果node为terminal\n",
    "            string_node = str(node['type'])+'=$$='+str(node['value']) ## + '==' + str(node['id'])\n",
    "            terminalCountMap[string_node] += 1\n",
    "        else:  # if it is a non-terminal\n",
    "            string_node = str(node['type']) + '=$$=' + \\\n",
    "                          str(node['hasSibling']) + '=$$=' + \\\n",
    "                          str(node['isTerminal']) # + '==' +str(node['id'])\n",
    "            if 'value' in node.keys(): # Note:There are some non-terminal contains 'value', use it?\n",
    "                string_node += '=$$=' + str(node['value'])\n",
    "            nonTerminalSet.add(string_node)\n",
    "            # todo: display the set of nonTerSet\n",
    "        return string_node\n",
    "\n",
    "    def in_order_traversal(data, index):\n",
    "        node = data[index]\n",
    "        if 'left' in node.keys():\n",
    "            in_order_traversal(data, node['left'])\n",
    "        # 如果该节点为non-terminal，则构建NT-pair并加入到sequence中。\n",
    "        if 'isTerminal' in node.keys() and node['isTerminal'] == False:\n",
    "            '''如果该node是non-terminal\n",
    "            如果该non-terminal包含一个terminal 子节点，则和该子节点组成NT_pair保存在output中\n",
    "            否则将NT_pair的T设为字符串EMPTY'''\n",
    "            N_pair = node_to_string(node)\n",
    "            T_pair = None\n",
    "            if data[node['left']]['isTerminal']==True:\n",
    "                assert data[node['left']]['id'] == node['left']\n",
    "                T_pair = node_to_string(data[node['left']])\n",
    "            else:\n",
    "                T_pair = node_to_string('EMPTY')\n",
    "            NT_pair = (N_pair, T_pair)\n",
    "            output.append(NT_pair)\n",
    "        else:\n",
    "            node_to_string(node)\n",
    "        # 遍历right side\n",
    "        if node['right'] != -1:\n",
    "            in_order_traversal(data, node['right'])\n",
    "    output = []\n",
    "    in_order_traversal(bi_tree, 0)\n",
    "    return output\n",
    "\n",
    "output = ast_to_seq(ast_example)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameter_dir = 'split_js_data/parameter.p'\n",
    "def load_dict_parameter():\n",
    "    file = open(data_parameter_dir, 'rb')\n",
    "    terminalToken2int, terminalInt2token, nonTerminalToken2int, nonTerminalInt2token = pickle.load(file)\n",
    "    return terminalToken2int, terminalInt2token, nonTerminalToken2int, nonTerminalInt2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminalToken2int, terminalInt2token, nonTerminalToken2int, nonTerminalInt2token = load_dict_parameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "30001\n"
     ]
    }
   ],
   "source": [
    "terminalToken2int, terminalInt2token, nonTerminalToken2int, nonTerminalInt2token = load_dict_parameter()\n",
    "print(len(nonTerminalInt2token))\n",
    "print(len(terminalInt2token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378700\n",
      "part1 of nt_seq data has been encoded into integer and saved...\n",
      "There are 378700 of nt_pair in train data set...\n",
      "295950\n",
      "part2 of nt_seq data has been encoded into integer and saved...\n",
      "There are 674650 of nt_pair in train data set...\n",
      "360650\n",
      "part3 of nt_seq data has been encoded into integer and saved...\n",
      "There are 1035300 of nt_pair in train data set...\n",
      "330050\n",
      "part4 of nt_seq data has been encoded into integer and saved...\n",
      "There are 1365350 of nt_pair in train data set...\n",
      "297200\n",
      "part5 of nt_seq data has been encoded into integer and saved...\n",
      "There are 1662550 of nt_pair in train data set...\n",
      "359500\n",
      "part6 of nt_seq data has been encoded into integer and saved...\n",
      "There are 2022050 of nt_pair in train data set...\n",
      "350400\n",
      "part7 of nt_seq data has been encoded into integer and saved...\n",
      "There are 2372450 of nt_pair in train data set...\n",
      "326050\n",
      "part8 of nt_seq data has been encoded into integer and saved...\n",
      "There are 2698500 of nt_pair in train data set...\n",
      "315200\n",
      "part9 of nt_seq data has been encoded into integer and saved...\n",
      "There are 3013700 of nt_pair in train data set...\n",
      "605000\n",
      "part10 of nt_seq data has been encoded into integer and saved...\n",
      "There are 3618700 of nt_pair in train data set...\n",
      "362750\n",
      "part11 of nt_seq data has been encoded into integer and saved...\n",
      "There are 3981450 of nt_pair in train data set...\n",
      "355550\n",
      "part12 of nt_seq data has been encoded into integer and saved...\n",
      "There are 4337000 of nt_pair in train data set...\n",
      "311500\n",
      "part13 of nt_seq data has been encoded into integer and saved...\n",
      "There are 4648500 of nt_pair in train data set...\n",
      "281050\n",
      "part14 of nt_seq data has been encoded into integer and saved...\n",
      "There are 4929550 of nt_pair in train data set...\n",
      "349650\n",
      "part15 of nt_seq data has been encoded into integer and saved...\n",
      "There are 5279200 of nt_pair in train data set...\n",
      "331900\n",
      "part16 of nt_seq data has been encoded into integer and saved...\n",
      "There are 5611100 of nt_pair in train data set...\n",
      "379250\n",
      "part17 of nt_seq data has been encoded into integer and saved...\n",
      "There are 5990350 of nt_pair in train data set...\n",
      "281150\n",
      "part18 of nt_seq data has been encoded into integer and saved...\n",
      "There are 6271500 of nt_pair in train data set...\n",
      "343700\n",
      "part19 of nt_seq data has been encoded into integer and saved...\n",
      "There are 6615200 of nt_pair in train data set...\n",
      "355700\n",
      "part20 of nt_seq data has been encoded into integer and saved...\n",
      "There are 6970900 of nt_pair in train data set...\n"
     ]
    }
   ],
   "source": [
    "def process_nt_seq(time_steps=50):\n",
    "    '''\n",
    "    对已经处理好的NT seq进行进一步的处理，\n",
    "    首先将每个token转换为number，然后截取各个seq成50的倍数，（50为time-steps大小）\n",
    "    然后将每个AST都拼接到一起，\n",
    "    '''\n",
    "    terminalToken2int, terminalInt2token, nonTerminalToken2int, nonTerminalInt2token = load_dict_parameter()\n",
    "    num_subset_train_data = 20\n",
    "    subset_data_dir = 'split_js_data/train_data/'\n",
    "    total_num_nt_pair = 0\n",
    "    \n",
    "    def get_subset_data(): #对每个part的nt_sequence读取并返回，等待进行处理\n",
    "        for i in range(1, num_subset_train_data + 1):\n",
    "            data_path = subset_data_dir + f'part{i}.json'\n",
    "            file = open(data_path, 'rb')\n",
    "            data = pickle.load(file)\n",
    "            yield (i, data)\n",
    "\n",
    "    subset_generator = get_subset_data()\n",
    "    for index, data in subset_generator:\n",
    "#        data = pickle.load(open(subset_data_dir+'part1.json', 'rb'))\n",
    "        data_seq = []\n",
    "        for one_ast in data: # 将每个nt_seq进行截取，并encode成integer，然后保存\n",
    "            num_steps = len(one_ast) // time_steps # 将每个nt seq都切割成time steps的整数倍\n",
    "            if num_steps == 0: # 该ast大小不足time step 舍去\n",
    "                continue\n",
    "            one_ast = one_ast[:num_steps * time_steps]\n",
    "            nt_int_seq = [(nonTerminalToken2int[n], terminalToken2int.get(t, terminalToken2int['UNK'])) \n",
    "                          for n, t in one_ast]\n",
    "            data_seq.extend(nt_int_seq)\n",
    "        print(len(data_seq))\n",
    "        total_num_nt_pair += len(data_seq)\n",
    "        with open(subset_data_dir+f'int_format/part{index}.json', 'wb') as file:\n",
    "            pickle.dump(data_seq, file)\n",
    "            print(f'part{index} of nt_seq data has been encoded into integer and saved...')\n",
    "    print(f'There are {total_num_nt_pair} of nt_pair in train data set...') # total == 6970900\n",
    "process_nt_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import data_utils\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = 'processed_data/rnn_train_data.p'\n",
    "data_parameter_path = 'processed_data/rnn_train_parameter.p'\n",
    "tensorboard_log_path = 'logs/MultiRNN'\n",
    "\n",
    "train_dir = 'dataset/programs_800'\n",
    "test_dir = 'dataset/programs_200'\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "\n",
    "num_epoches = 1\n",
    "show_every_n = 50\n",
    "save_every_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_tokens(train_flag=True, is_simplify=True):\n",
    "    if train_flag:\n",
    "        token_dir = train_dir\n",
    "    else:\n",
    "        token_dir = test_dir\n",
    "    token_list = []\n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            #print(file_path)\n",
    "            token_seq = json.load(open(file_path, encoding='utf-8'))\n",
    "            token_list.extend(token_seq)\n",
    "    string_token_list = []\n",
    "    for token in token_list:\n",
    "        if is_simplify:\n",
    "            data_utils.simplify_token(token)\n",
    "        string_token = data_utils.token_to_string(token)\n",
    "        string_token_list.append(string_token)\n",
    "    token_set = list(set(string_token_list))\n",
    "    #print(string_token_list[:10])\n",
    "    string2int = {c:i for i,c in enumerate(token_set)}\n",
    "    int2string = {i:c for i,c in enumerate(token_set)}\n",
    "    int_token_list = [string2int[c] for c in string_token_list]\n",
    "    #print(int_token_list[:10])\n",
    "    pickle.dump((int_token_list), open(processed_data_path, 'wb'))\n",
    "    pickle.dump((string2int, int2string, token_set), open(data_parameter_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using MultiRNN to pridect token. with LSTM cell\n",
    "'''\n",
    "class LSTM_Model(object):\n",
    "    def __init__(self,\n",
    "                 token_set, time_steps=100,\n",
    "                 batch_size=64,\n",
    "                 num_layers=2,\n",
    "                 n_units=128,\n",
    "                 learning_rate=0.003,\n",
    "                 grad_clip=5,\n",
    "                 keep_prob=0.5,\n",
    "                 num_epoches = 5,\n",
    "                 is_training=True):\n",
    "        \n",
    "        if is_training:\n",
    "            self.time_steps = time_steps\n",
    "            self.batch_size = batch_size\n",
    "        else:\n",
    "            self.time_steps = 1\n",
    "            self.batch_size = 1\n",
    "        \n",
    "        self.token_set =  token_set\n",
    "        self.num_classes = len(self.token_set)\n",
    "        self.num_layers = num_layers\n",
    "        self.n_units = n_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "        self.keep_prob = keep_prob\n",
    "        self.num_epoches = num_epoches\n",
    "\n",
    "        self.bulid_model()\n",
    "\n",
    "\n",
    "    def get_batch(self, data_seq, n_seq, n_steps):\n",
    "        '''\n",
    "        :param n_seq: 一个batch中序列的个数\n",
    "        :param n_steps: 单个序列中包含字符的个数\n",
    "        '''\n",
    "        data_seq = np.array(data_seq)\n",
    "        batch_size = n_steps * n_seq\n",
    "        n_batches = len(data_seq) // batch_size\n",
    "        data_seq = data_seq[:batch_size * n_batches] #仅保留完整的batch，舍去末尾\n",
    "        data_seq = data_seq.reshape((n_seq, -1))\n",
    "        for n in range(0, data_seq.shape[1], n_steps):\n",
    "            x = data_seq[:, n:n+n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:,0]\n",
    "            yield x, y\n",
    "\n",
    "    def build_input(self):\n",
    "        input_x = tf.placeholder(tf.int32, [self.batch_size, self.time_steps], name='input_x')\n",
    "        target_y = tf.placeholder(tf.int32, [self.batch_size, self.time_steps], name='target_y')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        return input_x, target_y, keep_prob\n",
    "\n",
    "    def bulid_lstm(self, keep_prob):\n",
    "        cell_list = []\n",
    "        for i in range(self.num_layers):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.n_units, state_is_tuple=True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "            cell_list.append(cell)\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(cell_list, state_is_tuple=True)\n",
    "        init_state = cells.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        return cells, init_state\n",
    "\n",
    "    def bulid_output(self, lstm_output):\n",
    "        # 将lstm_output的形状由[batch_size, time_steps, n_units] 转换为 [batch_size*time_steps, n_units]\n",
    "        seq_output = tf.concat(lstm_output, axis=1)\n",
    "        seq_output = tf.reshape(seq_output, [-1, self.n_units])\n",
    "\n",
    "        with tf.variable_scope('softmax'):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal([self.n_units, self.num_classes], stddev=0.1))\n",
    "            softmax_b = tf.Variable(tf.zeros(self.num_classes))\n",
    "\n",
    "        logits = tf.matmul(seq_output, softmax_w) + softmax_b\n",
    "        softmax_output = tf.nn.softmax(logits=logits, name='softmax_output')\n",
    "        return softmax_output, logits\n",
    "\n",
    "    def bulid_loss(self, logits, targets):\n",
    "        one_hot_y = tf.one_hot(targets, self.num_classes)\n",
    "        one_hot_y = tf.reshape(one_hot_y, logits.get_shape())\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def bulid_optimizer(self,loss):\n",
    "        # tvars = tf.trainable_variables()\n",
    "        # grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), self.grad_clip)\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        # optimizer = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradient_pairs = optimizer.compute_gradients(loss)\n",
    "        clip_gradient_pairs = []\n",
    "        for grad, var in gradient_pairs:\n",
    "            grad = tf.clip_by_value(grad, -2, 2)\n",
    "            clip_gradient_pairs.append((grad, var))\n",
    "        optimizer = optimizer.apply_gradients(clip_gradient_pairs)\n",
    "        return optimizer\n",
    "    \n",
    "    def build_accuracy(self, logits, targets):\n",
    "#         print(logits.get_shape())\n",
    "#         print(targets.get_shape())\n",
    "        sess = tf.Session()\n",
    "        self.show_logits = tf.argmax(logits, axis=1)\n",
    "        show_targets = tf.one_hot(targets, self.num_classes)\n",
    "        show_targets = tf.reshape(show_targets, logits.get_shape())\n",
    "        self.show_targets = tf.argmax(show_targets, axis=1)\n",
    "        self.aaa = tf.equal(self.show_logits, self.show_targets)\n",
    "        accu = tf.cast(self.aaa, tf.float32)\n",
    "        accu = tf.reduce_mean(accu)\n",
    "        return accu\n",
    "        \n",
    "\n",
    "    def bulid_model(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x, self.target_y, self.keep_prob = self.build_input()\n",
    "        self.cell, self.init_state = self.bulid_lstm(self.keep_prob)\n",
    "        one_hot_x = tf.one_hot(self.input_x, self.num_classes)\n",
    "        #print(one_hot_x.get_shape()) # (64, 100, 86)\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "            self.cell, one_hot_x, initial_state=self.init_state)\n",
    "        #print(1, lstm_outputs.get_shape()) # (64, 100, 128)\n",
    "        self.softmax_output, logits = self.bulid_output(lstm_outputs)\n",
    "        #print(self.softmax_output.get_shape()) # (6400, 86)\n",
    "        #print(logits.get_shape()) #(6400, 86)\n",
    "        self.loss = self.bulid_loss(logits,self.target_y)\n",
    "        self.accuracy = self.build_accuracy(self.softmax_output, self.target_y)\n",
    "        self.optimizer = self.bulid_optimizer(self.loss)\n",
    "\n",
    "\n",
    "    def train(self, data, string2int, int2string):\n",
    "        print('training begin...')\n",
    "        self.string2int = string2int\n",
    "        self.int2string = int2string\n",
    "        saver = tf.train.Saver(max_to_keep=100)\n",
    "        keep_prob = 0.5\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            global_step = 0\n",
    "            for epoch in range(self.num_epoches):\n",
    "                new_state = sess.run(self.init_state)\n",
    "                batch_generator = self.get_batch(data, self.batch_size, self.time_steps)\n",
    "                batch_step = 0\n",
    "                start_time = time.time()\n",
    "                for x, y in batch_generator:\n",
    "                    global_step += 1\n",
    "                    batch_step += 1\n",
    "                    feed = {self.input_x:x,\n",
    "                            self.target_y:y,\n",
    "                            self.keep_prob:keep_prob,\n",
    "                            self.init_state:new_state}\n",
    "                    show_accu, show_loss, new_state, _ = sess.run(\n",
    "                        [self.accuracy, self.loss, self.final_state, self.optimizer], feed_dict=feed)\n",
    "                    end_time = time.time()\n",
    "                    if global_step%show_every_n == 0:\n",
    "                        a, b,c = sess.run([self.show_logits, self.show_targets,self.aaa], feed)\n",
    "                        print(a[:10])\n",
    "                        print(b[:10])\n",
    "                    if global_step % show_every_n == 0:\n",
    "                        print('epoch: {}/{}..'.format(epoch+1, self.num_epoches),\n",
    "                              'global_step: {}..'.format(global_step),\n",
    "                              'train_loss: {:.2f}..'.format(show_loss),\n",
    "                              'train_accuracy: {:.2f}..'.format(show_accu),\n",
    "                              'time cost in per_batch: {:.2f}..'.format(end_time-start_time))\n",
    "\n",
    "                    if global_step % save_every_n == 0:\n",
    "                        saver.save(sess, 'checkpoints/epoch{}_batch_step{}'.format(epoch, batch_step))\n",
    "            saver.save(sess, 'checkpoints/last_check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.load_data_with_pickle(processed_data_path)\n",
    "string2int, int2string, token_set = data_utils.load_data_with_pickle(data_parameter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int2string[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTM_Model(token_set)\n",
    "model.train(train_data, string2int, int2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(object):\n",
    "    def __init__(self, token_set,string2int, int2string):\n",
    "        self.model = LSTM_Model(token_set, is_training=False)\n",
    "        self.string2int = string2int\n",
    "        self.int2string = int2string\n",
    "        self.last_chackpoints = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        '''\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, self.last_chackpoints)\n",
    "            new_state = sess.run(self.model.init_state)\n",
    "            prediction = None\n",
    "            for i,token in enumerate(prefix):\n",
    "                x = np.zeros((1, 1), dtype=np.int32)\n",
    "                x[0,0] = token\n",
    "                feed = {self.model.input_x:x,\n",
    "                        self.model.keep_prob:1.,\n",
    "                        self.model.init_state:new_state}\n",
    "                prediction, new_state = sess.run(\n",
    "                    [self.model.softmax_output, self.model.final_state], feed_dict=feed)\n",
    "        prediction = self.int2string[np.argmax(prediction)]\n",
    "        return prediction\n",
    "\n",
    "    def test(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prefix = self.token_to_int(prefix)\n",
    "            prediction = self.query_test(prefix, suffix)\n",
    "            prediction = data_utils.string_to_token(prediction)\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy\n",
    "    \n",
    "    def token_to_int(self, token_seq):\n",
    "        int_token_seq = []\n",
    "        for token in token_seq:\n",
    "            int_token = self.string2int[data_utils.token_to_string(token)]\n",
    "            int_token_seq.append(int_token)\n",
    "        return int_token_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_utils.load_data_with_file(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = TestModel(token_set, string2int, int2string)\n",
    "accuracy = test_model.test(test_data)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import data_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "使用TensorFlow自带的layers构建基本的神经网络对token进行预测，\n",
    "可以声明使用多少个context tokens 进行预测\n",
    "\n",
    "多个previous token输入神经网络的方法有两种想法：\n",
    "1. 将每个token的representation vector相连，合成一个大的vector输入到神经网络，\n",
    "    所以说神经网络的输入层大小应为：每个token vector length * number of previous token\n",
    "2. 应为目前表示每个token 使用的方法为one hot encoding，也就是说对每个token都是有且仅有一位为1，其余位为0，\n",
    "    所以可以考虑直接将所有的previous token相加，这样做的好处是NN输入层大小永远等于vector length。缺点是没有理论依据，不知道效果是否会更好\n",
    "\n",
    "\n",
    "1. concatenate the representations of previous tokens to a huge vector representation\n",
    "2. add the representations of previous tokens together\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "\n",
    "tensorboard_data_path = './logs/MultiContext/5_previous'\n",
    "\n",
    "epoch_num = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "context_size = 5\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code_Completion_Model:\n",
    "\n",
    "    def __init__(self, x_data, y_data, token_set, string2int, int2string, add_or_concat='add'):\n",
    "        batch_num = len(x_data) // batch_size\n",
    "        x_data, y_data = np.array(x_data[:batch_num * batch_size]), np.array(y_data[:batch_num * batch_size])\n",
    "        if add_or_concat == 'add':\n",
    "            temp_x, temp_y = self.reshape_with_add(x_data, y_data)\n",
    "        if add_or_concat == 'concat':\n",
    "            temp_x, temp_y = self.reshape_with_concat(x_data, y_data)\n",
    "        self.x_data, self.valid_x, self.y_data, self.valid_y = \\\n",
    "            train_test_split(temp_x, temp_y, train_size=0.9)\n",
    "        self.data_size = len(self.x_data)\n",
    "        self.index_to_string = int2string\n",
    "        self.string_to_index = string2int\n",
    "        self.tokens_set = token_set\n",
    "        self.tokens_size = len(token_set)\n",
    "\n",
    "    def reshape_with_concat(self, x_data, y_data):\n",
    "        reshape_data = []\n",
    "        reshape_label = []\n",
    "        for i in range(len(x_data)):\n",
    "            if i >= context_size-1:\n",
    "                temp_list = []\n",
    "                for x in range(context_size):\n",
    "                    temp_list.extend(x_data[i-x])\n",
    "                reshape_data.append(temp_list)\n",
    "                reshape_label.append(y_data[i])\n",
    "        return reshape_data, reshape_label\n",
    "        \n",
    "    def reshape_with_add(self, x_data, y_data):\n",
    "        x = []\n",
    "        y = []\n",
    "        for index, token in enumerate(x_data):\n",
    "            if index >= context_size - 1:\n",
    "                tempTokens = np.sum(x_data[index - context_size + 1:index + 1, :], axis=0)\n",
    "                x.append(tempTokens)\n",
    "                y.append(y_data[index])\n",
    "        return x, y;\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='input_x')\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='output_y')\n",
    "        weights = {'h1': tf.Variable(tf.truncated_normal(shape=[self.tokens_size, hidden_size])),\n",
    "                   'h2': tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size])),\n",
    "                   'h3': tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size])),\n",
    "                   'output': tf.Variable(tf.truncated_normal(shape=[hidden_size, self.tokens_size]))}\n",
    "        biases = {'h1': tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'h2': tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'h3': tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'output': tf.Variable(tf.constant(0.1, shape=[self.tokens_size], dtype=tf.float32))}\n",
    "\n",
    "        h1_layer = tf.matmul(self.input_x, weights['h1']) + biases['h1']\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        h2_layer = tf.matmul(h1_layer, weights['h2']) + biases['h2']\n",
    "        h2_layer = tf.nn.relu(h2_layer)\n",
    "        h3_layer = tf.matmul(h2_layer, weights['h3']) + biases['h3']\n",
    "        h3_layer = tf.nn.relu(h3_layer)\n",
    "        output_layer = tf.matmul(h3_layer, weights['output']) + biases['output']\n",
    "        self.prediction = tf.argmax(output_layer, 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=self.output_y)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        self.optimizer_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        equal = tf.equal(tf.argmax(output_layer, 1), tf.argmax(self.output_y, 1))\n",
    "        accuracy = tf.cast(equal, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(accuracy)\n",
    "        \n",
    "#         self.valid_loss = tf.reduce_mean(loss)\n",
    "#         self.valid_accuracy = tf.reduce_mean(accuracy)\n",
    "\n",
    "        tf.summary.histogram('weight1', weights['h1'])\n",
    "        tf.summary.histogram('weight2', weights['h2'])\n",
    "        tf.summary.histogram('output_weight', weights['output'])\n",
    "        tf.summary.histogram('bias1', biases['h1'])\n",
    "        tf.summary.histogram('bias2', biases['h2'])\n",
    "        tf.summary.histogram('output_bias', biases['output'])\n",
    "        tf.summary.scalar('train_loss', self.loss)\n",
    "        tf.summary.scalar('train_accuracy', self.accuracy)\n",
    "         \n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "    def get_batch(self):\n",
    "        for i in range(0, len(self.x_data), batch_size):\n",
    "            batch_x = self.x_data[i:i + batch_size];\n",
    "            batch_y = self.y_data[i:i + batch_size];\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def train(self):\n",
    "        self.create_NN()\n",
    "        self.sess = tf.Session()\n",
    "        valid_accu_list = np.zeros(10, dtype=np.float32)\n",
    "        train_accu_list = np.zeros(10, dtype=np.float32)\n",
    "        valid_list_index = 0\n",
    "        train_list_index = 0\n",
    "        writer = tf.summary.FileWriter(tensorboard_data_path, self.sess.graph)\n",
    "        time_begin = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epoch_num):\n",
    "          #  self.x_data, self.y_data = shuffle(self.x_data, self.y_data)\n",
    "            batch_generator = self.get_batch()\n",
    "            for i in range(0, len(self.x_data), batch_size):\n",
    "                batch_x, batch_y = next(batch_generator)\n",
    "                feed = {self.input_x: batch_x, self.output_y: batch_y}\n",
    "                _, summary_str = self.sess.run([self.optimizer_op, self.merged], feed_dict=feed)\n",
    "                writer.add_summary(summary_str, epoch*self.data_size + i)\n",
    "                writer.flush()\n",
    "                if (i // batch_size) % 2000 == 0:\n",
    "                    print('epoch: %d, step: %d'%(epoch, i))\n",
    "                    train_loss, train_accu = self.sess.run([self.loss, self.accuracy], feed_dict=feed)\n",
    "                    train_accu_list[train_list_index % 10] = train_accu\n",
    "                    print('train loss: %.2f, train accuracy:%.3f' % (train_loss, train_accu))\n",
    "                    print('average train accuracy: %.4f'%(np.mean(train_accu_list)))\n",
    "                    valid_feed = {self.input_x:self.valid_x, self.output_y:self.valid_y}\n",
    "                    valid_loss, valid_acc = self.sess.run([self.loss, self.accuracy], feed_dict=valid_feed)\n",
    "                    valid_accu_list[valid_list_index % 10] = valid_acc\n",
    "                    print('valid loss: %.2f, valid accuracy:%.3f' % (valid_loss, valid_acc))\n",
    "                    print('average valid accuracy: %.4f'%(np.mean(valid_accu_list)))\n",
    "        time_end = time.time()\n",
    "        print('training time cost: %.3f s' % (time_end - time_begin))\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole. In this function, use only one token before hole token to predict\n",
    "        '''\n",
    "        previous_token_list = prefix[-context_size:]\n",
    "        context_representation = np.zeros(self.tokens_size)\n",
    "\n",
    "        for token in previous_token_list:\n",
    "            prev_token_string = data_utils.token_to_string(token)\n",
    "            pre_token_x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "            context_representation += np.array(pre_token_x)\n",
    "\n",
    "        feed = {self.input_x: [context_representation]}\n",
    "        prediction = self.sess.run(self.prediction, feed)[0]\n",
    "        best_string = self.index_to_string[prediction]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    # test model\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prediction = self.query_test(prefix, suffix)[0]\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "x_data = data_utils.load_data_with_pickle(x_train_data_path)\n",
    "y_data = data_utils.load_data_with_pickle(y_train_data_path)\n",
    "token_set, string2int, int2string = data_utils.load_data_with_pickle(train_data_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model train\n",
    "model = Code_Completion_Model(x_data, y_data, token_set, string2int, int2string)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "query_test_data = data_utils.load_data_with_file(query_dir)\n",
    "accuracy = model.test_model(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import data_utils\n",
    "string_processed_data_path = 'processed_data/str_train_data.p'\n",
    "\n",
    "class Markov_Model(object):\n",
    "\n",
    "    def __init__(self, max_length=1, is_most=False):\n",
    "        self.markov_table = {}\n",
    "        self.max_length = 1\n",
    "        self.is_most = False\n",
    "\n",
    "    def create_model(self, token_list, max_depth=1, is_most=False):\n",
    "        '''\n",
    "        create a markov model with the depth from 1 to max_depth\n",
    "        {\n",
    "            depth1:{\n",
    "                key1:[value1, value2 ..]\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "        self.is_most = is_most\n",
    "        self.max_length = max_depth\n",
    "        for depth in range(1, max_depth+1):\n",
    "            temp_table = {}\n",
    "            for index in range(depth, len(token_list)):\n",
    "                words = tuple(token_list[index-depth:index])\n",
    "                if words in temp_table.keys():\n",
    "                    temp_table[words].append(token_list[index])\n",
    "                else:\n",
    "                    temp_table.setdefault(words, []).append(token_list[index])\n",
    "            if is_most:\n",
    "                for key,value in temp_table.items():\n",
    "                    temp = Counter(value).most_common(1)[0][0]\n",
    "                    temp_table[key] = temp\n",
    "                self.markov_table[depth] = temp_table\n",
    "            else:\n",
    "                self.markov_table[depth] = temp_table\n",
    "        return self.markov_table\n",
    "\n",
    "    def test_model(self, test_token_lists, depth=1):\n",
    "        correct = 0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "\n",
    "        for tokens in test_token_lists:\n",
    "            prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "            prediction = self.query_test(prefix, depth=depth)\n",
    "            if prediction['type']==expection[0]['type'] and prediction['value'] == expection[0]['value']:\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(test_token_lists)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def query_test(self, pre_tokens, depth=1):\n",
    "        while(depth>self.max_length):\n",
    "            depth -= 1\n",
    "        used_tokens = pre_tokens[-depth:]\n",
    "        proceed_tokens = []\n",
    "        for token in used_tokens:\n",
    "            proceed_tokens.append(data_utils.token_to_string(token))\n",
    "        proceed_tokens = tuple(proceed_tokens)\n",
    "        while proceed_tokens not in self.markov_table[depth].keys() and depth > 1:\n",
    "            depth -= 1\n",
    "            proceed_tokens = tuple(proceed_tokens[-depth:])\n",
    "\n",
    "        if self.is_most:\n",
    "            candidate = self.markov_table[depth][proceed_tokens]\n",
    "        else:\n",
    "            candidate_list = self.markov_table[depth][proceed_tokens]\n",
    "            random_index = random.randint(0, len(candidate_list)-1)\n",
    "            candidate = candidate_list[random_index]\n",
    "        prediction = data_utils.string_to_token(candidate)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_token_list = data_utils.load_data_with_pickle(string_processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = Markov_Model()\n",
    "markov_table = markov_model.create_model(string_token_list, max_depth=6, is_most=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(markov_table[1].keys())\n",
    "#print(markov_table[1][('Keyword~$$~var',)])\n",
    "#print(markov_table[2].keys())\n",
    "test_token_sequences = data_utils.load_data_with_file()\n",
    "accuracy = 0.0\n",
    "test_epoch = 10\n",
    "for i in range(test_epoch):\n",
    "    accuracy += markov_model.test_model(test_token_sequences, depth=6)\n",
    "accuracy /= test_epoch\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.arange(40).reshape((8,5))\n",
    "label = np.arange(8)\n",
    "print(data)\n",
    "print(label)\n",
    "size = 3\n",
    "reshape_data = []\n",
    "reshape_label = []\n",
    "for i in range(len(data)):\n",
    "    if i >= size-1:\n",
    "        temp_list = []\n",
    "        for x in range(size):\n",
    "            temp_list.extend(data[i-x])\n",
    "        reshape_data.append(temp_list)\n",
    "        reshape_label.append(label[i])\n",
    "        \n",
    "print(reshape_data)\n",
    "print(reshape_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
