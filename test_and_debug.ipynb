{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Completion System\n",
    "\n",
    "This is a JavaScript Code Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import data_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "'''\n",
    "使用TensorFlow自带的layers构建基本的神经网络对token进行预测，\n",
    "可以声明使用多少个context tokens 进行预测\n",
    "\n",
    "多个previous token输入神经网络的方法有两种想法：\n",
    "1. 将每个token的representation vector相连，合成一个大的vector输入到神经网络，\n",
    "    所以说神经网络的输入层大小应为：每个token vector length * number of previous token\n",
    "2. 应为目前表示每个token 使用的方法为one hot encoding，也就是说对每个token都是有且仅有一位为1，其余位为0，\n",
    "    所以可以考虑直接将所有的previous token相加，这样做的好处是NN输入层大小永远等于vector length。缺点是没有理论依据，不知道效果是否会更好\n",
    "\n",
    "\n",
    "1. concatenate the representations of previous tokens to a huge vector representation\n",
    "2. add the representations of previous tokens together\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "\n",
    "tensorboard_data_path = './logs/MultiContext'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "context_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "x_data = data_utils.load_data_with_pickle(x_train_data_path)\n",
    "y_data = data_utils.load_data_with_pickle(y_train_data_path)\n",
    "token_set, string2int, int2string = data_utils.load_data_with_pickle(train_data_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Code_Completion_Model:\n",
    "\n",
    "    def __init__(self, x_data, y_data, token_set, string2int, int2string):\n",
    "        batch_num = len(x_data) // batch_size\n",
    "        x_data, y_data = np.array(x_data[:batch_num*batch_size]), np.array(y_data[:batch_num*batch_size])\n",
    "        self.reshape_data(x_data, y_data)\n",
    "        self.x_data, self.valid_x, self.y_data, self.valid_y = \\\n",
    "            train_test_split(x_data, y_data, train_size=0.9)\n",
    "        self.index_to_string = int2string\n",
    "        self.string_to_index = string2int\n",
    "        self.tokens_set = token_set\n",
    "        self.tokens_size = len(token_set)\n",
    "\n",
    "    def reshape_data(self, x_data, y_data):\n",
    "        x = []\n",
    "        y = []\n",
    "        for index,token in enumerate(x_data):\n",
    "            if index >= context_size-1:\n",
    "                tempTokens = np.sum(x_data[index-context_size+1:index+1,:], axis=0)\n",
    "                x.append(tempTokens)\n",
    "                y.append(y_data[index])\n",
    "        return x, y;\n",
    "    \n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='input_x')\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='output_y')\n",
    "        weights = {'h1':tf.Variable(tf.truncated_normal(shape=[self.tokens_size, hidden_size])),\n",
    "                   'h2':tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size])),\n",
    "                   'output':tf.Variable(tf.truncated_normal(shape=[hidden_size, self.tokens_size]))}\n",
    "        biases = {'h1':tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'h2':tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'output':tf.Variable(tf.constant(0.1, shape=[self.tokens_size], dtype=tf.float32))}\n",
    "\n",
    "        h1_layer = tf.matmul(self.input_x, weights['h1']) + biases['h1']\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        h2_layer = tf.matmul(h1_layer, weights['h2']) + biases['h2']\n",
    "        h2_layer = tf.nn.relu(h2_layer)\n",
    "        output_layer = tf.matmul(h2_layer, weights['output']) + biases['output']\n",
    "        self.prediction = tf.argmax(output_layer, 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=self.output_y)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        self.optimizer_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        equal = tf.equal(tf.argmax(output_layer, 1), tf.argmax(self.output_y, 1))\n",
    "        accuracy = tf.cast(equal, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(accuracy)\n",
    "\n",
    "        tf.summary.histogram('weight1', weights['h1'])\n",
    "        tf.summary.histogram('weight2', weights['h2'])\n",
    "        tf.summary.histogram('output_weight', weights['output'])\n",
    "        tf.summary.histogram('bias1', biases['h1'])\n",
    "        tf.summary.histogram('bias2', biases['h2'])\n",
    "        tf.summary.histogram('output_bias', biases['output'])\n",
    "        tf.summary.scalar('train_loss', self.loss)\n",
    "        tf.summary.scalar('train_accuracy', self.accuracy)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "    def get_batch(self):\n",
    "        for i in range(0, len(self.x_data), batch_size):\n",
    "            batch_x = self.x_data[i:i+batch_size];\n",
    "            batch_y = self.y_data[i:i+batch_size];\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def train(self):\n",
    "        self.create_NN()\n",
    "        self.sess = tf.Session()\n",
    "        time_begin = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epoch_num):\n",
    "            batch_generator = self.get_batch()\n",
    "            for i in range(0, len(self.x_data), batch_size):\n",
    "                batch_x, batch_y = next(batch_generator)\n",
    "                feed = {self.input_x: batch_x, self.output_y: batch_y}\n",
    "                self.sess.run(self.optimizer_op, feed_dict=feed)\n",
    "                if (i // batch_size) % 2000 == 0:\n",
    "                    show_loss, show_acc = self.sess.run([self.loss, self.accuracy], feed_dict=feed)\n",
    "                    print('epoch: %d, training_step: %d, loss: %.2f, accuracy:%.3f' % (epoch, i, show_loss, show_acc))\n",
    "        time_end = time.time()\n",
    "        print('training time cost: %.3f s' % (time_end - time_begin))\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole. In this function, use only one token before hole token to predict\n",
    "        '''\n",
    "        previous_token_list = prefix[-previous_token_num:]\n",
    "        context_representation = np.zeros(self.tokens_size)\n",
    "\n",
    "        for token in previous_token_list:\n",
    "            prev_token_string = data_utils.token_to_string(token)\n",
    "            pre_token_x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "            context_representation += np.array(pre_token_x)\n",
    "\n",
    "        feed = {self.input_x: [context_representation]}\n",
    "        prediction = self.sess.run(self.prediction_index, feed)[0]\n",
    "        best_string = self.index_to_string[prediction]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    # test model\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prediction = self.query_test(prefix, suffix)[0]\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training_step: 0, loss: 87.87, accuracy:0.000\n",
      "epoch: 0, training_step: 128000, loss: 2.08, accuracy:0.359\n",
      "epoch: 0, training_step: 256000, loss: 1.45, accuracy:0.469\n",
      "epoch: 0, training_step: 384000, loss: 1.91, accuracy:0.391\n",
      "epoch: 0, training_step: 512000, loss: 1.45, accuracy:0.531\n",
      "epoch: 0, training_step: 640000, loss: 1.79, accuracy:0.453\n",
      "epoch: 0, training_step: 768000, loss: 1.69, accuracy:0.438\n",
      "epoch: 0, training_step: 896000, loss: 1.87, accuracy:0.391\n",
      "epoch: 0, training_step: 1024000, loss: 1.74, accuracy:0.438\n",
      "epoch: 0, training_step: 1152000, loss: 1.82, accuracy:0.344\n",
      "epoch: 0, training_step: 1280000, loss: 1.62, accuracy:0.422\n",
      "epoch: 0, training_step: 1408000, loss: 1.74, accuracy:0.453\n",
      "epoch: 0, training_step: 1536000, loss: 1.64, accuracy:0.516\n",
      "training time cost: 34.140 s\n"
     ]
    }
   ],
   "source": [
    "model = Code_Completion_Model(x_data, y_data, token_set, string2int, int2string)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Code_Completion_Model' object has no attribute 'prediction_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-c387446e1fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test epoch: %d, query test accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-57d1ddc68f9d>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(self, query_test_data)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken_sequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_test_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_hole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_equals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-57d1ddc68f9d>\u001b[0m in \u001b[0;36mquery_test\u001b[0;34m(self, prefix, suffix)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontext_representation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mbest_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mbest_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Code_Completion_Model' object has no attribute 'prediction_index'"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "query_test_data = data_utils.load_data_with_file(query_dir)\n",
    "test_accuracy = 0.0\n",
    "for i in range(test_epoch):\n",
    "    accuracy = model.test_model(query_test_data)\n",
    "    print('test epoch: %d, query test accuracy: %.3f' % (i, accuracy))\n",
    "    test_accuracy += accuracy\n",
    "print('total test accuracy: %.3f' % (test_accuracy / test_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(6,2)\n",
    "b = np.arange(6).reshape(6,1)\n",
    "N = 2\n",
    "x = []\n",
    "y = []\n",
    "for index,temp in enumerate(a):\n",
    "    if index >= N-1:\n",
    "        tempV = np.sum(a[index-N+1:index+1,:], axis=0)\n",
    "        x.append(tempV)\n",
    "        y.append(b[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]]\n",
      "[array([2, 4]), array([6, 8]), array([10, 12]), array([14, 16]), array([18, 20])]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module\n",
    "optimization idea:\n",
    "- re-implement dnn model with tensorflow(not tflearn)\n",
    "- using embedding method rather thant one_hot_encoding\n",
    "- using a deeper and wider network\n",
    "- using LSTM\n",
    "- training model not with only one previous token, severl tokens? and following tokens?\n",
    "- try CNN\n",
    "- see each source code file as a training batch, do not combine them as a huge training data(for RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_file = 'trained_model_parameter'\n",
    "\n",
    "epoch_num = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def load_tokens(token_dir, is_simplify=True):\n",
    "    '''\n",
    "    load token sequence data from input path: token_dir.\n",
    "    is_simplify: whether or not simplify the value of some variable type(see function for detail)\n",
    "    return a list whose elements are lists of a token sequence\n",
    "    '''\n",
    "    token_files = []  # stored the file's path which ends with 'tokens.json'\n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            token_files.append(file_path)\n",
    "\n",
    "    # load to a list, element is a token sequence of source code\n",
    "    token_lists = [json.load(open(f, encoding='utf-8')) for f in token_files]\n",
    "\n",
    "    def simplify_token(token):\n",
    "        '''\n",
    "        Because there are too many values for type: \"Identifier\", \"String\", \"Numeric\",\n",
    "        NN may be diffcult to train because of these different value.\n",
    "        So this function can transform these types of variables to a common value\n",
    "        '''\n",
    "        if token['type'] == 'Identifier':\n",
    "            token['value'] = 'id'\n",
    "        elif token['type'] == 'Numeric':\n",
    "            token['value'] = '1'\n",
    "        elif token['type'] == 'String':\n",
    "            token['value'] = 'string'\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if is_simplify:\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                simplify_token(token)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return token_lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building,\n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        time_begin = time.time()\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_size = len(self.tokens_set)  # 213\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_list)}\n",
    "        time_end = time.time()\n",
    "        print('model initialization time cost: ', time_end - time_begin)\n",
    "\n",
    "    # data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type': tokens[0], 'value': tokens[1]}\n",
    "\n",
    "    # encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self, string):\n",
    "        vector = [0] * self.tokens_size\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        print('data processing is begining...')\n",
    "        for token_sequence in self.token_lists:  # token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):  # each token(type_value) in source code\n",
    "                if index > 0:\n",
    "                    token_string = self.token_to_string(token)\n",
    "                    prev_token = self.token_to_string(token_sequence[index - 1])\n",
    "                    x_data.append(self.one_hot_encoding(prev_token))\n",
    "                    y_data.append(self.one_hot_encoding(token_string))\n",
    "        print('data processing is finished..')\n",
    "        pickle.dump((x_data, y_data), open('processed_data/saved_data_for_basic.p', 'wb'))\n",
    "        return x_data, y_data\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "#         self.input_x = tf.layers.Input(shape=[self.tokens_size])\n",
    "#         self.output_y = tf.layers.Input(shape=[self.tokens_size])\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size])\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size])\n",
    "        self.nn = tf.layers.dense(inputs=self.input_x, units=128, activation=tf.nn.relu)\n",
    "        self.output = tf.layers.dense(inputs=self.nn, units=self.tokens_size, activation=None)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.output_y)\n",
    "        self.loss = tf.reduce_sum(self.loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n",
    "        self.equal = tf.equal(tf.argmax(self.output_y,1), tf.argmax(self.output, 1))\n",
    "        self.accuarcy = tf.reduce_mean(tf.cast(self.equal, tf.float32))\n",
    "\n",
    "\n",
    "    # training ML model\n",
    "    def train(self, use_saved_data=False):\n",
    "        time_begin = time.time()\n",
    "        if use_saved_data:\n",
    "            x_data, y_data = pickle.load(open('processed_data/saved_data_for_basic.p', 'rb'))\n",
    "        else:\n",
    "            x_data, y_data = self.data_processing()\n",
    "            \n",
    "        time_end = time.time()\n",
    "        print('data processing time cost: ', time_end - time_begin)\n",
    "        self.create_NN()\n",
    "        time_begin = time.time()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(epoch_num):\n",
    "                for i in range(0, len(x_data), batch_size):\n",
    "                    batch_x = x_data[i:i+batch_size]\n",
    "                    batch_y = y_data[i:i+batch_size]\n",
    "                    feed = {self.input_x:batch_x, self.output_y:batch_y}\n",
    "                    sess.run(self.optimizer, feed_dict=feed)\n",
    "                    if (i//batch_size) % 500 == 0:\n",
    "                        show_acc = sess.run(self.accuarcy, feed_dict=feed)\n",
    "                        print('epoch: %d, training_step: %d, accuracy:%.3f'%(epoch, i, show_acc))\n",
    "\n",
    "        time_end = time.time()\n",
    "        print('training time cost: ', time_end - time_begin)\n",
    "        return time_end - time_begin\n",
    "\n",
    "\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = self.token_to_string(prefix[-1])\n",
    "        x = self.one_hot_encoding(prev_token_string)\n",
    "        with tf.Session() as sess:\n",
    "            feed = {self.input_x:x}\n",
    "            predict_list = sess.run(self.output, feed_dict=feed)\n",
    "            prediction = tf.argmax(predict_list, 1)\n",
    "            best_string = self.index_to_string[prediction]\n",
    "            best_token = self.string_to_token(best_string)\n",
    "        return [best_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_tokens(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "code_completion = Code_Completion_Model(dataset)\n",
    "use_stored_model = False\n",
    "\n",
    "train_time = code_completion.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "\n",
    "\n",
    "import data_utils\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_dir = 'saved_model/model_parameter'\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building,\n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.string_to_index, self.index_to_string, token_set = \\\n",
    "            data_utils.load_data_with_pickle('processed_data/train_parameter.p')\n",
    "        self.num_token = len(token_set)\n",
    "\n",
    "        \n",
    "    def init_with_orig_data(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        self.dataset = token_lists\n",
    "        self.tokens_set = data_utils.get_token_set(self.dataset)\n",
    "        self.num_tokens = len(self.tokens_set)  # 74 经过简化后只有74种token\n",
    "        print(self.num_tokens)\n",
    "        # 构建映射字典\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_set)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_set)}\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for index, token in enumerate(self.dataset):\n",
    "            if index > 0:\n",
    "                token_string = data_utils.token_to_string(token)\n",
    "                prev_token = data_utils.token_to_string(self.dataset[index - 1])\n",
    "                x_data.append(self.one_hot_encoding(prev_token))\n",
    "                y_data.append(self.one_hot_encoding(token_string))\n",
    "        return x_data, y_data\n",
    "\n",
    "    def vector_data_process(self,dataset):\n",
    "        '''\n",
    "        读取已经被处理成one_hot_vector的token data，该函数会根据该dataset\n",
    "        构造x_data and y_data\n",
    "        :param dataset:\n",
    "        :return:\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for index, token in enumerate(dataset):\n",
    "            if index > 0:\n",
    "                x_data.append(dataset[index])\n",
    "                y_data.append(token)\n",
    "        return x_data, y_data\n",
    "\n",
    "\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.nn = tflearn.input_data(shape=[None, self.num_token])\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, self.num_token, activation='softmax')\n",
    "        self.nn = tflearn.regression(self.nn)\n",
    "        self.model = tflearn.DNN(self.nn)\n",
    "\n",
    "    # load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "\n",
    "    # training ML model\n",
    "    def train(self, train_data, with_original_data=False):\n",
    "        print('model training...')\n",
    "        if with_original_data:\n",
    "            self.init_with_orig_data(train_data)\n",
    "            x_data, y_data = self.data_processing()\n",
    "            self.create_NN()\n",
    "            self.model.fit(x_data, y_data, n_epoch=1, batch_size=500, show_metric=True)\n",
    "        else:\n",
    "            x_data, y_data = self.vector_data_process(train_data)\n",
    "            self.create_NN()\n",
    "            self.model.fit(\n",
    "                x_data, y_data, n_epoch=1, validation_set=0.2, batch_size=500, show_metric=True)\n",
    "\n",
    "    # save trained model to model path\n",
    "    def save_model(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = data_utils.token_to_string(prefix[-1])\n",
    "        x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "        y = self.model.predict([x])\n",
    "        predicted_seq = y[0]\n",
    "        if type(predicted_seq) is np.ndarray:\n",
    "            predicted_seq = predicted_seq.tolist()\n",
    "        best_number = predicted_seq.index(max(predicted_seq))\n",
    "        print('prediction:', best_number)\n",
    "        best_string = self.index_to_string[best_number]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for tokens in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "            prediction = self.query_test(prefix, suffix)\n",
    "\n",
    "            if data_utils.token_equals(prediction, expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_utils.load_data_with_pickle('processed_data/vec_train_data.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load and model create\n",
    "cc_model = Code_Completion_Model()\n",
    "#training model\n",
    "use_stored_model = False\n",
    "if use_stored_model:\n",
    "    cc_model.load_model(model_dir)\n",
    "else:\n",
    "    cc_model.train(processed_data, with_original_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_data = data_utils.load_data_with_file(query_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_test_haha(query_test_data):\n",
    "    '''\n",
    "    Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "    ML model will predict the most probable token in the hole\n",
    "    In this function, use only one token before hole token to predict\n",
    "    return: the most probable token\n",
    "    '''\n",
    "    correct = 0\n",
    "    correct_token_list = []\n",
    "    incorrect_token_list = []\n",
    "    for tokens in query_test_data:\n",
    "        prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "        prediction = cc_model.query_test(prefix, suffix)\n",
    "        \n",
    "        strring = data_utils.token_to_string(expection[0])\n",
    "        #print(strring)\n",
    "        index_num = cc_model.string_to_index[strring]\n",
    "        print('expection:', index_num)\n",
    "        print('\\n')\n",
    "        if data_utils.token_equals(prediction, expection):\n",
    "            correct += 1\n",
    "            correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        else:\n",
    "            incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "    accuracy = correct / len(query_test_data)\n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "使用TensorFlow自带的layers构建基本的神经网络对token进行预测，\n",
    "可以声明使用多少个context tokens 进行预测\n",
    "\n",
    "多个previous token输入神经网络的方法有两种想法：\n",
    "1. 将每个token的representation vector相连，合成一个大的vector输入到神经网络，\n",
    "    所以说神经网络的输入层大小应为：每个token vector length * number of previous token\n",
    "2. 应为目前表示每个token 使用的方法为one hot encoding，也就是说对每个token都是有且仅有一位为1，其余位为0，\n",
    "    所以可以考虑直接将所有的previous token相加，这样做的好处是NN输入层大小永远等于vector length。缺点是没有理论依据，不知道效果是否会更好\n",
    "\n",
    "\n",
    "1. concatenate the representations of previous tokens to a huge vector representation\n",
    "2. add the representations of previous tokens together\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "\n",
    "epoch_num = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "previous_token_num = 2\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "\n",
    "    def __init__(self, x_data, y_data, token_set, string2int, int2string):\n",
    "        batch_num = len(x_data) // batch_size\n",
    "        self.x_data = x_data[:batch_num * batch_size]\n",
    "        self.y_data = y_data[:batch_num * batch_size]\n",
    "        self.index_to_string = int2string\n",
    "        self.string_to_index = string2int\n",
    "        self.tokens_set = token_set\n",
    "        self.tokens_size = len(token_set)\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='input_x')\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='output_y')\n",
    "        self.nn = tf.layers.dense(inputs=self.input_x, units=128, activation=tf.nn.relu, name='hidden_1')\n",
    "        self.output = tf.layers.dense(inputs=self.nn, units=self.tokens_size, activation=None, name='prediction')\n",
    "        self.prediction_index = tf.argmax(self.output, 1)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.output_y, name='loss')\n",
    "        self.loss = tf.reduce_sum(self.loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n",
    "        self.equal = tf.equal(tf.argmax(self.output_y, 1), tf.argmax(self.output, 1))\n",
    "        self.accuarcy = tf.reduce_mean(tf.cast(self.equal, tf.float32), name='accuracy')\n",
    "\n",
    "    def get_batch(self, context_size = previous_token_num):\n",
    "        \n",
    "        x_data = np.array(self.x_data)\n",
    "        for i in range(0, len(self.x_data), batch_size):\n",
    "            batch_x = np.zeros((batch_size, self.tokens_size))\n",
    "            for j in range(context_size):\n",
    "                if i >= j:\n",
    "                    temp = x_data[i-j:i-j+batch_size].reshape(-1, self.tokens_size)\n",
    "                    if temp.shape == (0, 86): break;\n",
    "                    batch_x += temp\n",
    "            batch_y = self.y_data[i:i + batch_size]\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def train(self):\n",
    "        self.create_NN()\n",
    "        self.sess = tf.Session()\n",
    "        time_begin = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        batch_generator = self.get_batch()\n",
    "        for epoch in range(epoch_num):\n",
    "            for i in range(0, len(self.x_data), batch_size):\n",
    "                batch_x, batch_y = next(batch_generator)\n",
    "                feed = {self.input_x: batch_x, self.output_y: batch_y}\n",
    "                self.sess.run(self.optimizer, feed_dict=feed)\n",
    "                if (i // batch_size) % 2000 == 0:\n",
    "                    show_loss, show_acc = self.sess.run([self.loss, self.accuarcy], feed_dict=feed)\n",
    "                    print('epoch: %d, training_step: %d, loss: %.2f, accuracy:%.3f' % (epoch, i, show_loss, show_acc))\n",
    "        time_end = time.time()\n",
    "        print('training time cost: %.3f s'%(time_end - time_begin))\n",
    "\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole. In this function, use only one token before hole token to predict\n",
    "        '''\n",
    "        previous_token_list = prefix[-previous_token_num:]\n",
    "        context_representation = np.zeros(self.tokens_size)\n",
    "\n",
    "        for token in previous_token_list:\n",
    "            prev_token_string = data_utils.token_to_string(token)\n",
    "            pre_token_x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "            context_representation += np.array(pre_token_x)\n",
    "\n",
    "        feed = {self.input_x: [context_representation]}\n",
    "        prediction = self.sess.run(self.prediction_index, feed)[0]\n",
    "        best_string = self.index_to_string[prediction]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    #test model\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prediction = self.query_test(prefix, suffix)[0]\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "x_data = data_utils.load_data_with_pickle(x_train_data_path)\n",
    "y_data = data_utils.load_data_with_pickle(y_train_data_path)\n",
    "token_set, string2int, int2string = data_utils.load_data_with_pickle(train_data_parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model train\n",
    "model = Code_Completion_Model(x_data, y_data, token_set, string2int, int2string)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "query_test_data = data_utils.load_data_with_file(query_dir)\n",
    "accuracy = model.test_model(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import data_utils\n",
    "\n",
    "\n",
    "'''\n",
    "使用TensorFlow自带的layers构建基本的神经网络对token进行预测，预测只使用前一个token\n",
    "'''\n",
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "\n",
    "tensorboard_data_path = './tensorboard_data/model3'\n",
    "\n",
    "query_dir = 'dataset/programs_200/'\n",
    "\n",
    "epoch_num = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "test_epoch = 3\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code_Completion_Model:\n",
    "\n",
    "    def __init__(self, x_data, y_data, token_set, string2int, int2string):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.x_data, self.valid_x, self.y_data, self.valid_y = \\\n",
    "                train_test_split(x_data, y_data, train_size=0.9, random_state=100)\n",
    "        self.index_to_string = int2string\n",
    "        self.string_to_index = string2int\n",
    "        self.tokens_set = token_set\n",
    "        self.tokens_size = len(token_set)\n",
    "        self.data_size = len(self.x_data)\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='input_x')\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='output_y')\n",
    "        weights = {'h1':tf.Variable(tf.truncated_normal(shape=[self.tokens_size, hidden_size])),\n",
    "                   'h2':tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size])),\n",
    "                   'output':tf.Variable(tf.truncated_normal(shape=[hidden_size, self.tokens_size]))}\n",
    "        biases = {'h1':tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'h2':tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'output':tf.Variable(tf.constant(0.1, shape=[self.tokens_size], dtype=tf.float32))}\n",
    "\n",
    "        h1_layer = tf.matmul(self.input_x, weights['h1']) + biases['h1']\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        h2_layer = tf.matmul(h1_layer, weights['h2']) + biases['h2']\n",
    "        h2_layer = tf.nn.relu(h2_layer)\n",
    "        output_layer = tf.matmul(h2_layer, weights['output']) + biases['output']\n",
    "        self.prediction = tf.argmax(output_layer, 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=self.output_y)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        self.optimizer_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        equal = tf.equal(tf.argmax(output_layer, 1), tf.argmax(self.output_y, 1))\n",
    "        accuracy = tf.cast(equal, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(accuracy)\n",
    "\n",
    "        tf.summary.histogram('weight1', weights['h1'])\n",
    "        tf.summary.histogram('weight2', weights['h2'])\n",
    "        tf.summary.histogram('output_weight', weights['output'])\n",
    "        tf.summary.histogram('bias1', biases['h1'])\n",
    "        tf.summary.histogram('bias2', biases['h2'])\n",
    "        tf.summary.histogram('output_bias', biases['output'])\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('loss', self.accuracy)\n",
    "\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.create_NN()\n",
    "        self.sess = tf.Session()\n",
    "        writer = tf.summary.FileWriter(tensorboard_data_path, self.sess.graph)\n",
    "        time_begin = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epoch_num):\n",
    "            for i in range(0, self.data_size, batch_size):\n",
    "                batch_x = self.x_data[i:i + batch_size]\n",
    "                batch_y = self.y_data[i:i + batch_size]\n",
    "                feed = {self.input_x: batch_x, self.output_y: batch_y}\n",
    "                _, summary_str = self.sess.run([self.optimizer_op, self.merged], feed_dict=feed)\n",
    "                writer.add_summary(summary_str, epoch*self.data_size + i)\n",
    "                writer.flush()\n",
    "                if (i // batch_size) % 2000 == 0:\n",
    "                    valid_feed = {self.input_x:self.valid_x, self.output_y:self.valid_y}\n",
    "                    valid_loss, valid_acc = self.sess.run([self.loss, self.accuracy], feed_dict=valid_feed)\n",
    "                    show_loss, show_acc = self.sess.run([self.loss, self.accuracy], feed_dict=feed)\n",
    "                    print('epoch: %d, training_step: %d, loss: %.2f, accuracy:%.3f' % (epoch, i, show_loss, show_acc))\n",
    "                    print('epoch: %d, trianing_step: %d, valid: %.2f, accuracy:%.3f'%(epoch, i, valid_loss, valid_acc))\n",
    "        time_end = time.time()\n",
    "\n",
    "        print('training time cost: %.3f ms' %(time_end - time_begin))\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = data_utils.token_to_string(prefix[-1])\n",
    "        pre_token_x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "        feed = {self.input_x: [pre_token_x]}\n",
    "        prediction = self.sess.run(self.prediction_index, feed)[0]\n",
    "        best_string = self.index_to_string[prediction]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prediction = self.query_test(prefix, suffix)[0]\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data_utils.load_data_with_pickle(x_train_data_path)\n",
    "y_data = data_utils.load_data_with_pickle(y_train_data_path)\n",
    "token_set, string2int, int2string = data_utils.load_data_with_pickle(train_data_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training_step: 0, loss: 86.34, accuracy:0.008\n",
      "epoch: 0, trianing_step: 0, valid: 86.34, accuracy:0.002\n",
      "epoch: 0, training_step: 256000, loss: 1.61, accuracy:0.422\n",
      "epoch: 0, trianing_step: 256000, valid: 1.92, accuracy:0.361\n",
      "epoch: 0, training_step: 512000, loss: 1.75, accuracy:0.430\n",
      "epoch: 0, trianing_step: 512000, valid: 1.75, accuracy:0.439\n",
      "epoch: 0, training_step: 768000, loss: 1.46, accuracy:0.547\n",
      "epoch: 0, trianing_step: 768000, valid: 1.70, accuracy:0.450\n",
      "epoch: 0, training_step: 1024000, loss: 1.58, accuracy:0.453\n",
      "epoch: 0, trianing_step: 1024000, valid: 1.69, accuracy:0.450\n",
      "epoch: 0, training_step: 1280000, loss: 1.60, accuracy:0.492\n",
      "epoch: 0, trianing_step: 1280000, valid: 1.69, accuracy:0.451\n",
      "epoch: 0, training_step: 1536000, loss: 1.85, accuracy:0.352\n",
      "epoch: 0, trianing_step: 1536000, valid: 1.68, accuracy:0.448\n",
      "training time cost: 76.916 ms\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Code_Completion_Model' object has no attribute 'keep_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ae3e84a20104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test epoch: %d, query test accuracy: %.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fdd28980e689>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(self, query_test_data)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken_sequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_test_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_hole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_equals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fdd28980e689>\u001b[0m in \u001b[0;36mquery_test\u001b[0;34m(self, prefix, suffix)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprev_token_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpre_token_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_token_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpre_token_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mbest_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Code_Completion_Model' object has no attribute 'keep_prob'"
     ]
    }
   ],
   "source": [
    "#model train\n",
    "model = Code_Completion_Model(x_data, y_data, token_set, string2int, int2string)\n",
    "model.train()\n",
    "\n",
    "# test model\n",
    "query_test_data = data_utils.load_data_with_file(query_dir)\n",
    "test_accuracy = 0.0\n",
    "for i in range(test_epoch):\n",
    "    accuracy = model.test_model(query_test_data)\n",
    "    print('test epoch: %d, query test accuracy: %.3f'%(i, accuracy))\n",
    "    test_accuracy += accuracy\n",
    "print('total test accuracy: %.3f'%(test_accuracy/test_epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
