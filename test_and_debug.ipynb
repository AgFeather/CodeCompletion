{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Completion System\n",
    "\n",
    "This is a JavaScript Code Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_file = 'trained_model_parameter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tokens from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(token_dir, is_simplify=True):\n",
    "    '''\n",
    "    load token sequence data from input path: token_dir.\n",
    "    is_simplify: whether or not simplify the value of some variable type(see function for detail)\n",
    "    return a list whose elements are lists of a token sequence\n",
    "    '''\n",
    "    token_files = [] #stored the file's path which ends with 'tokens.json' \n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            token_files.append(file_path)\n",
    "            \n",
    "   #load to a list, element is a token sequence of source code         \n",
    "    token_lists = [json.load(open(f, encoding='utf-8')) for f in token_files]\n",
    "    def simplify_token(token):\n",
    "        '''\n",
    "        Because there are too many values for type: \"Identifier\", \"String\", \"Numeric\",\n",
    "        NN may be diffcult to train because of these different value. \n",
    "        So this function can transform these types of variables to a common value\n",
    "        '''\n",
    "        if token['type'] == 'Identifier':\n",
    "            token['value'] = 'id'\n",
    "        elif token['type'] == 'Numeric':\n",
    "            token['value'] = '1'\n",
    "        elif token['type'] == 'String':\n",
    "            token['value'] = 'string'\n",
    "        else:\n",
    "            pass\n",
    "    if is_simplify:\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                simplify_token(token)\n",
    "    else:\n",
    "        pass        \n",
    "    \n",
    "    return token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building, \n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        time_begin = time.time()\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_size = len(self.tokens_set) #213\n",
    "        self.index_to_string = {i:s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s:i for i, s in enumerate(self.tokens_list)}\n",
    "        time_end =time.time()\n",
    "        print('model initialization time cost: ', time_end - time_begin)\n",
    "    \n",
    "        \n",
    "    #data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type':tokens[0], 'value':tokens[1]}\n",
    "    \n",
    "    #encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self,string):\n",
    "        vector = [0] * self.tokens_size\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "    \n",
    "    #generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        print('data processing is begining...')\n",
    "        for token_sequence in self.token_lists:#token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):#each token(type_value) in source code\n",
    "                if index > 0:\n",
    "                    token_string = self.token_to_string(token)\n",
    "                    prev_token = self.token_to_string(token_sequence[index - 1])\n",
    "                    x_data.append(self.one_hot_encoding(prev_token))\n",
    "                    y_data.append(self.one_hot_encoding(token_string))\n",
    "        print('data processing is finished..')\n",
    "        return x_data, y_data\n",
    "    \n",
    "    #neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.nn = tflearn.input_data(shape=[None, self.tokens_size])\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128, activation='')\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, self.tokens_size, activation='softmax')\n",
    "        self.nn = tflearn.regression(self.nn)\n",
    "        self.model = tflearn.DNN(self.nn)\n",
    "    \n",
    "    #load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "    \n",
    "    #training ML model\n",
    "    def train(self):\n",
    "        time_begin = time.time()\n",
    "        x_data, y_data = self.data_processing()\n",
    "        time_end = time.time()\n",
    "        print('data processing time cost: ', time_end - time_begin)\n",
    "        self.create_NN()\n",
    "        time_begin = time.time()\n",
    "        self.model.fit(x_data, y_data, n_epoch=1, batch_size=500, show_metric = True)\n",
    "        time_end = time.time()\n",
    "        print('training time cost: ', time_end - time_begin)\n",
    "        return time_end - time_begin\n",
    "        \n",
    "    #save trained model to model path\n",
    "    def save_model(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "        \n",
    "    #query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token \n",
    "        '''\n",
    "        prev_token_string = self.token_to_string(prefix[-1])\n",
    "        x = self.one_hot_encoding(prev_token_string)\n",
    "        y = self.model.predict([x])\n",
    "        predicted_seq = y[0]\n",
    "        if type(predicted_seq) is np.ndarray:\n",
    "            predicted_seq = predicted_seq.tolist()\n",
    "        best_number = predicted_seq.index(max(predicted_seq))\n",
    "        best_string = self.index_to_string[best_number]\n",
    "        best_token = self.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing is finished..\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dataset = load_tokens(train_dir)\n",
    "code_completion = Code_Completion_Model(dataset)\n",
    "use_stored_model = False\n",
    "if use_stored_model:\n",
    "    code_completion.load_model(model_file)\n",
    "else:\n",
    "    train_time = code_completion.train()\n",
    "    code_completion.save_model(model_file)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('total time cost: %.2f s, model training cost: %.2f s'%(end_time-start_time, train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hole(tokens, max_hole_size = 2):\n",
    "    '''\n",
    "    input: a tokens sequence of source code and max_hole_size\n",
    "    return: hole token to be predicted (expection)\n",
    "            token sequence before the hole(prefix)\n",
    "            token sequence after the hole(suffix)\n",
    "    '''\n",
    "    hole_size = min(random.randint(1, max_hole_size), len(tokens) - 1)\n",
    "    hole_start_index = random.randint(1, len(tokens) - hole_size)\n",
    "    hole_end_index = hole_start_index + hole_size\n",
    "    prefix = tokens[0 : hole_start_index]\n",
    "    expection = tokens[hole_start_index : hole_end_index]\n",
    "    suffix = tokens[hole_end_index : 0]\n",
    "    return prefix, expection, suffix\n",
    "\n",
    "def token_equals(token1, token2):\n",
    "    '''\n",
    "    Determining whether input two tokens are equal or not\n",
    "    '''\n",
    "    if len(token1) != len(token2):\n",
    "        return False\n",
    "    for index, t1 in enumerate(token1):\n",
    "        t2 = token2[index]\n",
    "        if t1['type'] != t2['type'] or t1['value'] != t2['value']:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query test accuracy:  0.16\n"
     ]
    }
   ],
   "source": [
    "query_test_data = load_tokens(query_dir)\n",
    "correct = 0\n",
    "correct_token_list = []\n",
    "incorrect_token_list = []\n",
    "for tokens in query_test_data:\n",
    "    prefix, expection, suffix = create_hole(tokens)\n",
    "    prediction = code_completion.query_test(prefix, suffix)\n",
    "    if token_equals(prediction, expection):\n",
    "        correct += 1\n",
    "        correct_token_list.append({'expection':expection, 'prediction':prediction})\n",
    "    else:\n",
    "        incorrect_token_list.append({'expection':expection, 'prediction':prediction})\n",
    "accuracy = correct / len(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct_token_list: \\n', correct_token_list[:5])\n",
    "print('incorrect_token_list: \\n', incorrect_token_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module\n",
    "optimization idea:\n",
    "- re-implement dnn model with tensorflow(not tflearn)\n",
    "- using embedding method rather thant one_hot_encoding\n",
    "- using a deeper and wider network\n",
    "- using LSTM\n",
    "- training model not with only one previous token, severl tokens? and following tokens?\n",
    "- try CNN\n",
    "- see each source code file as a training batch, do not combine them as a huge training data(for RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_file = 'trained_model_parameter'\n",
    "\n",
    "epoch_num = 1\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def load_tokens(token_dir, is_simplify=True):\n",
    "    '''\n",
    "    load token sequence data from input path: token_dir.\n",
    "    is_simplify: whether or not simplify the value of some variable type(see function for detail)\n",
    "    return a list whose elements are lists of a token sequence\n",
    "    '''\n",
    "    token_files = []  # stored the file's path which ends with 'tokens.json'\n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            token_files.append(file_path)\n",
    "\n",
    "    # load to a list, element is a token sequence of source code\n",
    "    token_lists = [json.load(open(f, encoding='utf-8')) for f in token_files]\n",
    "\n",
    "    def simplify_token(token):\n",
    "        '''\n",
    "        Because there are too many values for type: \"Identifier\", \"String\", \"Numeric\",\n",
    "        NN may be diffcult to train because of these different value.\n",
    "        So this function can transform these types of variables to a common value\n",
    "        '''\n",
    "        if token['type'] == 'Identifier':\n",
    "            token['value'] = 'id'\n",
    "        elif token['type'] == 'Numeric':\n",
    "            token['value'] = '1'\n",
    "        elif token['type'] == 'String':\n",
    "            token['value'] = 'string'\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if is_simplify:\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                simplify_token(token)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return token_lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building,\n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        time_begin = time.time()\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_size = len(self.tokens_set)  # 213\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_list)}\n",
    "        time_end = time.time()\n",
    "        print('model initialization time cost: ', time_end - time_begin)\n",
    "\n",
    "    # data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type': tokens[0], 'value': tokens[1]}\n",
    "\n",
    "    # encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self, string):\n",
    "        vector = [0] * self.tokens_size\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        print('data processing is begining...')\n",
    "        for token_sequence in self.token_lists:  # token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):  # each token(type_value) in source code\n",
    "                if index > 0:\n",
    "                    token_string = self.token_to_string(token)\n",
    "                    prev_token = self.token_to_string(token_sequence[index - 1])\n",
    "                    x_data.append(self.one_hot_encoding(prev_token))\n",
    "                    y_data.append(self.one_hot_encoding(token_string))\n",
    "        print('data processing is finished..')\n",
    "        pickle.dump((x_data, y_data), open('processed_data/saved_data_for_basic.p', 'wb'))\n",
    "        return x_data, y_data\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "#         self.input_x = tf.layers.Input(shape=[self.tokens_size])\n",
    "#         self.output_y = tf.layers.Input(shape=[self.tokens_size])\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size])\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size])\n",
    "        self.nn = tf.layers.dense(inputs=self.input_x, units=128, activation=tf.nn.relu)\n",
    "        self.output = tf.layers.dense(inputs=self.nn, units=self.tokens_size, activation=None)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.output_y)\n",
    "        self.loss = tf.reduce_sum(self.loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n",
    "        self.equal = tf.equal(tf.argmax(self.output_y,1), tf.argmax(self.output, 1))\n",
    "        self.accuarcy = tf.reduce_mean(tf.cast(self.equal, tf.float32))\n",
    "\n",
    "\n",
    "    # training ML model\n",
    "    def train(self, use_saved_data=False):\n",
    "        time_begin = time.time()\n",
    "        if use_saved_data:\n",
    "            x_data, y_data = pickle.load(open('processed_data/saved_data_for_basic.p', 'rb'))\n",
    "        else:\n",
    "            x_data, y_data = self.data_processing()\n",
    "            \n",
    "        time_end = time.time()\n",
    "        print('data processing time cost: ', time_end - time_begin)\n",
    "        self.create_NN()\n",
    "        time_begin = time.time()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(epoch_num):\n",
    "                for i in range(0, len(x_data), batch_size):\n",
    "                    batch_x = x_data[i:i+batch_size]\n",
    "                    batch_y = y_data[i:i+batch_size]\n",
    "                    feed = {self.input_x:batch_x, self.output_y:batch_y}\n",
    "                    sess.run(self.optimizer, feed_dict=feed)\n",
    "                    if (i//batch_size) % 500 == 0:\n",
    "                        show_acc = sess.run(self.accuarcy, feed_dict=feed)\n",
    "                        print('epoch: %d, training_step: %d, accuracy:%.3f'%(epoch, i, show_acc))\n",
    "\n",
    "        time_end = time.time()\n",
    "        print('training time cost: ', time_end - time_begin)\n",
    "        return time_end - time_begin\n",
    "\n",
    "\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = self.token_to_string(prefix[-1])\n",
    "        x = self.one_hot_encoding(prev_token_string)\n",
    "        with tf.Session() as sess:\n",
    "            feed = {self.input_x:x}\n",
    "            predict_list = sess.run(self.output, feed_dict=feed)\n",
    "            prediction = tf.argmax(predict_list, 1)\n",
    "            best_string = self.index_to_string[prediction]\n",
    "            best_token = self.string_to_token(best_string)\n",
    "        return [best_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_tokens(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialization time cost:  0.8267886638641357\n",
      "data processing is begining...\n",
      "data processing is finished..\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4d9b12cb70f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muse_stored_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_completion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-f9ac002433d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, use_saved_data)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data/saved_data_for_basic.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-f9ac002433d5>\u001b[0m in \u001b[0;36mdata_processing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data processing is finished..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data/saved_data_for_basic.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "code_completion = Code_Completion_Model(dataset)\n",
    "use_stored_model = False\n",
    "\n",
    "train_time = code_completion.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "\n",
    "\n",
    "import data_utils\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_dir = 'saved_model/model_parameter'\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building,\n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.string_to_index, self.index_to_string, token_set = \\\n",
    "            data_utils.load_data_with_pickle('processed_data/train_parameter.p')\n",
    "        self.num_token = len(token_set)\n",
    "\n",
    "        \n",
    "    def init_with_orig_data(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        self.dataset = token_lists\n",
    "        self.tokens_set = data_utils.get_token_set(self.dataset)\n",
    "        self.num_tokens = len(self.tokens_set)  # 74 经过简化后只有74种token\n",
    "        print(self.num_tokens)\n",
    "        # 构建映射字典\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_set)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_set)}\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for index, token in enumerate(self.dataset):\n",
    "            if index > 0:\n",
    "                token_string = data_utils.token_to_string(token)\n",
    "                prev_token = data_utils.token_to_string(self.dataset[index - 1])\n",
    "                x_data.append(self.one_hot_encoding(prev_token))\n",
    "                y_data.append(self.one_hot_encoding(token_string))\n",
    "        return x_data, y_data\n",
    "\n",
    "    def vector_data_process(self,dataset):\n",
    "        '''\n",
    "        读取已经被处理成one_hot_vector的token data，该函数会根据该dataset\n",
    "        构造x_data and y_data\n",
    "        :param dataset:\n",
    "        :return:\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for index, token in enumerate(dataset):\n",
    "            if index > 0:\n",
    "                x_data.append(dataset[index])\n",
    "                y_data.append(token)\n",
    "        return x_data, y_data\n",
    "\n",
    "\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.nn = tflearn.input_data(shape=[None, self.num_token])\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, self.num_token, activation='softmax')\n",
    "        self.nn = tflearn.regression(self.nn)\n",
    "        self.model = tflearn.DNN(self.nn)\n",
    "\n",
    "    # load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "\n",
    "    # training ML model\n",
    "    def train(self, train_data, with_original_data=False):\n",
    "        print('model training...')\n",
    "        if with_original_data:\n",
    "            self.init_with_orig_data(train_data)\n",
    "            x_data, y_data = self.data_processing()\n",
    "            self.create_NN()\n",
    "            self.model.fit(x_data, y_data, n_epoch=1, batch_size=500, show_metric=True)\n",
    "        else:\n",
    "            x_data, y_data = self.vector_data_process(train_data)\n",
    "            self.create_NN()\n",
    "            self.model.fit(\n",
    "                x_data, y_data, n_epoch=1, validation_set=0.2, batch_size=500, show_metric=True)\n",
    "\n",
    "    # save trained model to model path\n",
    "    def save_model(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        prev_token_string = data_utils.token_to_string(prefix[-1])\n",
    "        x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "        y = self.model.predict([x])\n",
    "        predicted_seq = y[0]\n",
    "        if type(predicted_seq) is np.ndarray:\n",
    "            predicted_seq = predicted_seq.tolist()\n",
    "        best_number = predicted_seq.index(max(predicted_seq))\n",
    "        print('prediction:', best_number)\n",
    "        best_string = self.index_to_string[best_number]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for tokens in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "            prediction = self.query_test(prefix, suffix)\n",
    "\n",
    "            if data_utils.token_equals(prediction, expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_utils.load_data_with_pickle('processed_data/vec_train_data.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load and model create\n",
    "cc_model = Code_Completion_Model()\n",
    "#training model\n",
    "use_stored_model = False\n",
    "if use_stored_model:\n",
    "    cc_model.load_model(model_dir)\n",
    "else:\n",
    "    cc_model.train(processed_data, with_original_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_data = data_utils.load_data_with_file(query_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_test_haha(query_test_data):\n",
    "    '''\n",
    "    Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "    ML model will predict the most probable token in the hole\n",
    "    In this function, use only one token before hole token to predict\n",
    "    return: the most probable token\n",
    "    '''\n",
    "    correct = 0\n",
    "    correct_token_list = []\n",
    "    incorrect_token_list = []\n",
    "    for tokens in query_test_data:\n",
    "        prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "        prediction = cc_model.query_test(prefix, suffix)\n",
    "        \n",
    "        strring = data_utils.token_to_string(expection[0])\n",
    "        #print(strring)\n",
    "        index_num = cc_model.string_to_index[strring]\n",
    "        print('expection:', index_num)\n",
    "        print('\\n')\n",
    "        if data_utils.token_equals(prediction, expection):\n",
    "            correct += 1\n",
    "            correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        else:\n",
    "            incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "    accuracy = correct / len(query_test_data)\n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_haha(query_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.load_data_with_pickle('processed_data/vec_train_data.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2int, int2string, token_set = data_utils.load_data_with_pickle('processed_data/train_parameter.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = Code_Completion_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
