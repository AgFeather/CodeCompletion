{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Completion System\n",
    "\n",
    "This is a JavaScript Code Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import data_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "使用TensorFlow自带的layers构建基本的神经网络对token进行预测，\n",
    "可以声明使用多少个context tokens 进行预测\n",
    "\n",
    "多个previous token输入神经网络的方法有两种想法：\n",
    "1. 将每个token的representation vector相连，合成一个大的vector输入到神经网络，\n",
    "    所以说神经网络的输入层大小应为：每个token vector length * number of previous token\n",
    "2. 应为目前表示每个token 使用的方法为one hot encoding，也就是说对每个token都是有且仅有一位为1，其余位为0，\n",
    "    所以可以考虑直接将所有的previous token相加，这样做的好处是NN输入层大小永远等于vector length。缺点是没有理论依据，不知道效果是否会更好\n",
    "\n",
    "\n",
    "1. concatenate the representations of previous tokens to a huge vector representation\n",
    "2. add the representations of previous tokens together\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "\n",
    "tensorboard_data_path = './logs/MultiContext/5_previous'\n",
    "\n",
    "epoch_num = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "context_size = 5\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "\n",
    "    def __init__(self, x_data, y_data, token_set, string2int, int2string):\n",
    "        batch_num = len(x_data) // batch_size\n",
    "        x_data, y_data = np.array(x_data[:batch_num * batch_size]), np.array(y_data[:batch_num * batch_size])\n",
    "        self.reshape_data(x_data, y_data)\n",
    "        self.x_data, self.valid_x, self.y_data, self.valid_y = \\\n",
    "            train_test_split(x_data, y_data, train_size=0.9)\n",
    "        self.data_size = len(self.x_data)\n",
    "        self.index_to_string = int2string\n",
    "        self.string_to_index = string2int\n",
    "        self.tokens_set = token_set\n",
    "        self.tokens_size = len(token_set)\n",
    "\n",
    "    def reshape_data(self, x_data, y_data):\n",
    "        x = []\n",
    "        y = []\n",
    "        for index, token in enumerate(x_data):\n",
    "            if index >= context_size - 1:\n",
    "                tempTokens = np.sum(x_data[index - context_size + 1:index + 1, :], axis=0)\n",
    "                x.append(tempTokens)\n",
    "                y.append(y_data[index])\n",
    "        return x, y;\n",
    "\n",
    "    # neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='input_x')\n",
    "        self.output_y = tf.placeholder(dtype=tf.float32, shape=[None, self.tokens_size], name='output_y')\n",
    "        weights = {'h1': tf.Variable(tf.truncated_normal(shape=[self.tokens_size, hidden_size])),\n",
    "                   'h2': tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size])),\n",
    "                   'h3': tf.Variable(tf.truncated_normal(shape=[hidden_size, hidden_size])),\n",
    "                   'output': tf.Variable(tf.truncated_normal(shape=[hidden_size, self.tokens_size]))}\n",
    "        biases = {'h1': tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'h2': tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'h3': tf.Variable(tf.constant(0.1, shape=[hidden_size], dtype=tf.float32)),\n",
    "                  'output': tf.Variable(tf.constant(0.1, shape=[self.tokens_size], dtype=tf.float32))}\n",
    "\n",
    "        h1_layer = tf.matmul(self.input_x, weights['h1']) + biases['h1']\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        h2_layer = tf.matmul(h1_layer, weights['h2']) + biases['h2']\n",
    "        h2_layer = tf.nn.relu(h2_layer)\n",
    "        h3_layer = tf.matmul(h2_layer, weights['h3']) + biases['h3']\n",
    "        h3_layer = tf.nn.relu(h3_layer)\n",
    "        output_layer = tf.matmul(h3_layer, weights['output']) + biases['output']\n",
    "        self.prediction = tf.argmax(output_layer, 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=self.output_y)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        self.optimizer_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        equal = tf.equal(tf.argmax(output_layer, 1), tf.argmax(self.output_y, 1))\n",
    "        accuracy = tf.cast(equal, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(accuracy)\n",
    "        \n",
    "#         self.valid_loss = tf.reduce_mean(loss)\n",
    "#         self.valid_accuracy = tf.reduce_mean(accuracy)\n",
    "\n",
    "        tf.summary.histogram('weight1', weights['h1'])\n",
    "        tf.summary.histogram('weight2', weights['h2'])\n",
    "        tf.summary.histogram('output_weight', weights['output'])\n",
    "        tf.summary.histogram('bias1', biases['h1'])\n",
    "        tf.summary.histogram('bias2', biases['h2'])\n",
    "        tf.summary.histogram('output_bias', biases['output'])\n",
    "        tf.summary.scalar('train_loss', self.loss)\n",
    "        tf.summary.scalar('train_accuracy', self.accuracy)\n",
    "         \n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "    def get_batch(self):\n",
    "        for i in range(0, len(self.x_data), batch_size):\n",
    "            batch_x = self.x_data[i:i + batch_size];\n",
    "            batch_y = self.y_data[i:i + batch_size];\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def train(self):\n",
    "        self.create_NN()\n",
    "        self.sess = tf.Session()\n",
    "        valid_accu_list = np.zeros(10, dtype=np.float32)\n",
    "        train_accu_list = np.zeros(10, dtye=np.float32)\n",
    "        valid_list_index = 0\n",
    "        train_list_index = 0\n",
    "        writer = tf.summary.FileWriter(tensorboard_data_path, self.sess.graph)\n",
    "        time_begin = time.time()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epoch_num):\n",
    "          #  self.x_data, self.y_data = shuffle(self.x_data, self.y_data)\n",
    "            batch_generator = self.get_batch()\n",
    "            for i in range(0, len(self.x_data), batch_size):\n",
    "                batch_x, batch_y = next(batch_generator)\n",
    "                feed = {self.input_x: batch_x, self.output_y: batch_y}\n",
    "                _, summary_str = self.sess.run([self.optimizer_op, self.merged], feed_dict=feed)\n",
    "                writer.add_summary(summary_str, epoch*self.data_size + i)\n",
    "                writer.flush()\n",
    "                if (i // batch_size) % 2000 == 0:\n",
    "                    print('epoch: %d, step: %d'%(epoch, i))\n",
    "                    train_loss, train_accu = self.sess.run([self.loss, self.accuracy], feed_dict=feed)\n",
    "                    train_accu_list[train_list_index % 10] = train_accu\n",
    "                    print('train loss: %.2f, train accuracy:%.3f' % (train_loss, train_accu))\n",
    "                    print('average train accuracy: %.4f'%(np.mean(train_accu_list)))\n",
    "                    valid_feed = {self.input_x:self.valid_x, self.output_y:self.valid_y}\n",
    "                    valid_loss, valid_acc = self.sess.run([self.loss, self.accuracy], feed_dict=valid_feed)\n",
    "                    valid_accu_list[valid_list_index % 10] = valid_acc\n",
    "                    print('valid loss: %.2f, valid accuracy:%.3f' % (valid_loss, valid_acc))\n",
    "                    print('average valid accuracy: %.4f'%(np.mean(valid_accu_list)))\n",
    "        time_end = time.time()\n",
    "        print('training time cost: %.3f s' % (time_end - time_begin))\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole. In this function, use only one token before hole token to predict\n",
    "        '''\n",
    "        previous_token_list = prefix[-context_size:]\n",
    "        context_representation = np.zeros(self.tokens_size)\n",
    "\n",
    "        for token in previous_token_list:\n",
    "            prev_token_string = data_utils.token_to_string(token)\n",
    "            pre_token_x = data_utils.one_hot_encoding(prev_token_string, self.string_to_index)\n",
    "            context_representation += np.array(pre_token_x)\n",
    "\n",
    "        feed = {self.input_x: [context_representation]}\n",
    "        prediction = self.sess.run(self.prediction, feed)[0]\n",
    "        best_string = self.index_to_string[prediction]\n",
    "        best_token = data_utils.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "\n",
    "    # test model\n",
    "    def test_model(self, query_test_data):\n",
    "        correct = 0.0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "        for token_sequence in query_test_data:\n",
    "            prefix, expection, suffix = data_utils.create_hole(token_sequence)\n",
    "            prediction = self.query_test(prefix, suffix)[0]\n",
    "            if data_utils.token_equals([prediction], expection):\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(query_test_data)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data_path = 'processed_data/x_train_data.p'\n",
    "y_train_data_path = 'processed_data/y_train_data.p'\n",
    "train_data_parameter = 'processed_data/x_y_parameter.p'\n",
    "x_data = data_utils.load_data_with_pickle(x_train_data_path)\n",
    "y_data = data_utils.load_data_with_pickle(y_train_data_path)\n",
    "token_set, string2int, int2string = data_utils.load_data_with_pickle(train_data_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, train loss: 364.78, train accuracy:0.000\n",
      "average train accuracy: 0.00\n",
      "epoch: 0, step: 0, valid loss: 365.05, valid accuracy:0.006\n",
      "average valid accuracy: 0.00\n",
      "epoch: 0, step: 128000, train loss: 2.15, train accuracy:0.406\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 128000, valid loss: 2.18, valid accuracy:0.430\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 256000, train loss: 3.02, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 256000, valid loss: 1.99, valid accuracy:0.386\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 384000, train loss: 2.07, train accuracy:0.375\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 384000, valid loss: 1.92, valid accuracy:0.428\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 512000, train loss: 1.72, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 512000, valid loss: 1.89, valid accuracy:0.418\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 640000, train loss: 1.63, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 640000, valid loss: 1.84, valid accuracy:0.423\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 768000, train loss: 1.69, train accuracy:0.484\n",
      "average train accuracy: 0.05\n",
      "epoch: 0, step: 768000, valid loss: 1.78, valid accuracy:0.441\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 896000, train loss: 1.70, train accuracy:0.406\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 896000, valid loss: 1.74, valid accuracy:0.415\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 1024000, train loss: 1.65, train accuracy:0.406\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 1024000, valid loss: 1.73, valid accuracy:0.421\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 1152000, train loss: 1.96, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 1152000, valid loss: 1.71, valid accuracy:0.443\n",
      "average valid accuracy: 0.04\n",
      "epoch: 0, step: 1280000, train loss: 1.80, train accuracy:0.406\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 1280000, valid loss: 1.69, valid accuracy:0.452\n",
      "average valid accuracy: 0.05\n",
      "epoch: 0, step: 1408000, train loss: 1.56, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 1408000, valid loss: 1.69, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 0, step: 1536000, train loss: 1.61, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 0, step: 1536000, valid loss: 1.69, valid accuracy:0.446\n",
      "average valid accuracy: 0.04\n",
      "epoch: 1, step: 0, train loss: 1.63, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 0, valid loss: 1.68, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 128000, train loss: 1.83, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 128000, valid loss: 1.68, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 256000, train loss: 1.64, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 256000, valid loss: 1.68, valid accuracy:0.451\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 384000, train loss: 1.94, train accuracy:0.344\n",
      "average train accuracy: 0.03\n",
      "epoch: 1, step: 384000, valid loss: 1.68, valid accuracy:0.446\n",
      "average valid accuracy: 0.04\n",
      "epoch: 1, step: 512000, train loss: 1.54, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 1, step: 512000, valid loss: 1.68, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 640000, train loss: 1.55, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 1, step: 640000, valid loss: 1.68, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 768000, train loss: 1.66, train accuracy:0.484\n",
      "average train accuracy: 0.05\n",
      "epoch: 1, step: 768000, valid loss: 1.68, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 896000, train loss: 1.66, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 896000, valid loss: 1.68, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 1024000, train loss: 1.62, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 1, step: 1024000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 1152000, train loss: 1.93, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 1152000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 1280000, train loss: 1.80, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 1280000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 1408000, train loss: 1.54, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 1, step: 1408000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 1, step: 1536000, train loss: 1.60, train accuracy:0.453\n",
      "average train accuracy: 0.05\n",
      "epoch: 1, step: 1536000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 0, train loss: 1.67, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 0, valid loss: 1.68, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 128000, train loss: 1.85, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 128000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 256000, train loss: 1.64, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 2, step: 256000, valid loss: 1.67, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 384000, train loss: 1.93, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 384000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 512000, train loss: 1.56, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 2, step: 512000, valid loss: 1.67, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 640000, train loss: 1.57, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 2, step: 640000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 768000, train loss: 1.66, train accuracy:0.484\n",
      "average train accuracy: 0.05\n",
      "epoch: 2, step: 768000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 896000, train loss: 1.63, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 896000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 1024000, train loss: 1.60, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 2, step: 1024000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 1152000, train loss: 1.93, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 1152000, valid loss: 1.67, valid accuracy:0.453\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 1280000, train loss: 1.80, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 1280000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 1408000, train loss: 1.54, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 2, step: 1408000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 2, step: 1536000, train loss: 1.60, train accuracy:0.453\n",
      "average train accuracy: 0.05\n",
      "epoch: 2, step: 1536000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 0, train loss: 1.66, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 0, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 128000, train loss: 1.84, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 128000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 256000, train loss: 1.67, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 3, step: 256000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 384000, train loss: 1.94, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 384000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 512000, train loss: 1.56, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 3, step: 512000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 640000, train loss: 1.59, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 3, step: 640000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, step: 768000, train loss: 1.65, train accuracy:0.484\n",
      "average train accuracy: 0.05\n",
      "epoch: 3, step: 768000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 896000, train loss: 1.62, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 896000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 1024000, train loss: 1.60, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 3, step: 1024000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 1152000, train loss: 1.93, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 1152000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 1280000, train loss: 1.78, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 1280000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 1408000, train loss: 1.54, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 3, step: 1408000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 3, step: 1536000, train loss: 1.59, train accuracy:0.453\n",
      "average train accuracy: 0.05\n",
      "epoch: 3, step: 1536000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 0, train loss: 1.66, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 0, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 128000, train loss: 1.84, train accuracy:0.438\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 128000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 256000, train loss: 1.69, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 4, step: 256000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 384000, train loss: 1.94, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 384000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 512000, train loss: 1.56, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 4, step: 512000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 640000, train loss: 1.59, train accuracy:0.453\n",
      "average train accuracy: 0.05\n",
      "epoch: 4, step: 640000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 768000, train loss: 1.63, train accuracy:0.484\n",
      "average train accuracy: 0.05\n",
      "epoch: 4, step: 768000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 896000, train loss: 1.62, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 896000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 1024000, train loss: 1.60, train accuracy:0.469\n",
      "average train accuracy: 0.05\n",
      "epoch: 4, step: 1024000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 1152000, train loss: 1.92, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 1152000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 1280000, train loss: 1.78, train accuracy:0.391\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 1280000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 1408000, train loss: 1.54, train accuracy:0.422\n",
      "average train accuracy: 0.04\n",
      "epoch: 4, step: 1408000, valid loss: 1.67, valid accuracy:0.455\n",
      "average valid accuracy: 0.05\n",
      "epoch: 4, step: 1536000, train loss: 1.59, train accuracy:0.453\n",
      "average train accuracy: 0.05\n",
      "epoch: 4, step: 1536000, valid loss: 1.67, valid accuracy:0.454\n",
      "average valid accuracy: 0.05\n",
      "training time cost: 357.183 s\n"
     ]
    }
   ],
   "source": [
    "#model train\n",
    "model = Code_Completion_Model(x_data, y_data, token_set, string2int, int2string)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query test accuracy:  0.225\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "query_test_data = data_utils.load_data_with_file(query_dir)\n",
    "accuracy = model.test_model(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import data_utils\n",
    "string_processed_data_path = 'processed_data/str_train_data.p'\n",
    "\n",
    "class Markov_Model(object):\n",
    "\n",
    "    def __init__(self, max_length=1, is_most=False):\n",
    "        self.markov_table = {}\n",
    "        self.max_length = 1\n",
    "        self.is_most = False\n",
    "\n",
    "    def create_model(self, token_list, max_depth=1, is_most=False):\n",
    "        '''\n",
    "        create a markov model with the depth from 1 to max_depth\n",
    "        {\n",
    "            depth1:{\n",
    "                key1:[value1, value2 ..]\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "        self.is_most = is_most\n",
    "        self.max_length = max_depth\n",
    "        for depth in range(1, max_depth+1):\n",
    "            temp_table = {}\n",
    "            for index in range(depth, len(token_list)):\n",
    "                words = tuple(token_list[index-depth:index])\n",
    "                if words in temp_table.keys():\n",
    "                    temp_table[words].append(token_list[index])\n",
    "                else:\n",
    "                    temp_table.setdefault(words, []).append(token_list[index])\n",
    "            if is_most:\n",
    "                for key,value in temp_table.items():\n",
    "                    temp = Counter(value).most_common(1)[0][0]\n",
    "                    temp_table[key] = temp\n",
    "                self.markov_table[depth] = temp_table\n",
    "            else:\n",
    "                self.markov_table[depth] = temp_table\n",
    "        return self.markov_table\n",
    "\n",
    "    def test_model(self, test_token_lists, depth=1):\n",
    "        correct = 0\n",
    "        correct_token_list = []\n",
    "        incorrect_token_list = []\n",
    "\n",
    "        for tokens in test_token_lists:\n",
    "            prefix, expection, suffix = data_utils.create_hole(tokens)\n",
    "            prediction = self.query_test(prefix, depth=depth)\n",
    "            if prediction['type']==expection[0]['type'] and prediction['value'] == expection[0]['value']:\n",
    "                correct += 1\n",
    "                correct_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "            else:\n",
    "                incorrect_token_list.append({'expection': expection, 'prediction': prediction})\n",
    "        accuracy = correct / len(test_token_lists)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def query_test(self, pre_tokens, depth=1):\n",
    "        while(depth>self.max_length):\n",
    "            depth -= 1\n",
    "        used_tokens = pre_tokens[-depth:]\n",
    "        proceed_tokens = []\n",
    "        for token in used_tokens:\n",
    "            proceed_tokens.append(data_utils.token_to_string(token))\n",
    "        proceed_tokens = tuple(proceed_tokens)\n",
    "        while proceed_tokens not in self.markov_table[depth].keys() and depth > 1:\n",
    "            depth -= 1\n",
    "            proceed_tokens = tuple(proceed_tokens[-depth:])\n",
    "\n",
    "        if self.is_most:\n",
    "            candidate = self.markov_table[depth][proceed_tokens]\n",
    "        else:\n",
    "            candidate_list = self.markov_table[depth][proceed_tokens]\n",
    "            random_index = random.randint(0, len(candidate_list)-1)\n",
    "            candidate = candidate_list[random_index]\n",
    "        prediction = data_utils.string_to_token(candidate)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_token_list = data_utils.load_data_with_pickle(string_processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = Markov_Model()\n",
    "markov_table = markov_model.create_model(string_token_list, max_depth=6, is_most=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636\n"
     ]
    }
   ],
   "source": [
    "#print(markov_table[1].keys())\n",
    "#print(markov_table[1][('Keyword~$$~var',)])\n",
    "#print(markov_table[2].keys())\n",
    "test_token_sequences = data_utils.load_data_with_file()\n",
    "accuracy = 0.0\n",
    "test_epoch = 10\n",
    "for i in range(test_epoch):\n",
    "    accuracy += markov_model.test_model(test_token_sequences, depth=6)\n",
    "accuracy /= test_epoch\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sentence = 'asgasdfasdfadsfadsfads'\n",
    "con = Counter(sentence).most_common(1)[0][0]\n",
    "print(type(con))\n",
    "print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abaaaaab\n",
      "a\n",
      "dddddwwdwwdw\n",
      "d\n",
      "{'1': 'a', '2': 'd'}\n"
     ]
    }
   ],
   "source": [
    "mapp = {'1': 'abaaaaab', '2': 'dddddwwdwwdw'}\n",
    "for key,value in mapp.items():\n",
    "    temp = Counter(value).most_common(1)[0][0]\n",
    "    mapp[key] = temp\n",
    "print(mapp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
