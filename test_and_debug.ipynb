{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Completion System\n",
    "\n",
    "This is a JavaScript Code Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_file = 'trained_model_parameter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tokens from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(token_dir, is_simplify=True):\n",
    "    '''\n",
    "    load token sequence data from input path: token_dir.\n",
    "    is_simplify: whether or not simplify the value of some variable type(see function for detail)\n",
    "    return a list whose elements are lists of a token sequence\n",
    "    '''\n",
    "    token_files = [] #stored the file's path which ends with 'tokens.json' \n",
    "    for f in os.listdir(token_dir):\n",
    "        file_path = os.path.join(token_dir, f)\n",
    "        if os.path.isfile(file_path) and f.endswith('_tokens.json'):\n",
    "            token_files.append(file_path)\n",
    "            \n",
    "   #load to a list, element is a token sequence of source code         \n",
    "    token_lists = [json.load(open(f, encoding='utf-8')) for f in token_files]\n",
    "    def simplify_token(token):\n",
    "        '''\n",
    "        Because there are too many values for type: \"Identifier\", \"String\", \"Numeric\",\n",
    "        NN may be diffcult to train because of these different value. \n",
    "        So this function can transform these types of variables to a common value\n",
    "        '''\n",
    "        if token['type'] == 'Identifier':\n",
    "            token['value'] = 'id'\n",
    "        elif token['type'] == 'Numeric':\n",
    "            token['value'] = '1'\n",
    "        elif token['type'] == 'String':\n",
    "            token['value'] = 'string'\n",
    "        else:\n",
    "            pass\n",
    "    if is_simplify:\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                simplify_token(token)\n",
    "    else:\n",
    "        pass        \n",
    "    \n",
    "    return token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    '''\n",
    "    Machine Learning model class, including data processing, encoding, model_building, \n",
    "    training, query_testing, model_save, model_load\n",
    "    '''\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_size = len(self.tokens_set) #213\n",
    "        self.index_to_string = {i:s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s:i for i, s in enumerate(self.tokens_list)}\n",
    "        \n",
    "    #data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type':tokens[0], 'value':tokens[1]}\n",
    "    \n",
    "    #encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self,string):\n",
    "        vector = [0] * self.tokens_size\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "    \n",
    "    #generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        '''\n",
    "        first, transform a token in dict form to a type-value string\n",
    "        x_data is a token, y_label is the previous token of x_data\n",
    "        '''\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for token_sequence in self.token_lists:#token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):#each token(type_value) in source code\n",
    "                if index > 0:\n",
    "                    token_string = self.token_to_string(token)\n",
    "                    prev_token = self.token_to_string(token_sequence[index - 1])\n",
    "                    x_data.append(self.one_hot_encoding(prev_token))\n",
    "                    y_data.append(self.one_hot_encoding(token_string))\n",
    "        print('data processing is finished..')\n",
    "        return x_data, y_data\n",
    "    \n",
    "    #neural network functions\n",
    "    def create_NN(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.nn = tflearn.input_data(shape=[None, self.tokens_size])\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, 128)\n",
    "        self.nn = tflearn.fully_connected(self.nn, self.tokens_size, activation='softmax')\n",
    "        self.nn = tflearn.regression(self.nn)\n",
    "        self.model = tflearn.DNN(self.nn)\n",
    "    \n",
    "    #load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "    \n",
    "    #training ML model\n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        x_data, y_data = self.data_processing()\n",
    "        self.create_NN()\n",
    "        self.model.fit(x_data, y_data, n_epoch=1, batch_size=500, show_metric = True)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "        \n",
    "    #save trained model to model path\n",
    "    def save_model(self, model_file):\n",
    "        self.model.save(model_file)\n",
    "        \n",
    "    #query test\n",
    "    def query_test(self, prefix, suffix):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token \n",
    "        '''\n",
    "        prev_token_string = self.token_to_string(prefix[-1])\n",
    "        x = self.one_hot_encoding(prev_token_string)\n",
    "        y = self.model.predict([x])\n",
    "        predicted_seq = y[0]\n",
    "        if type(predicted_seq) is np.ndarray:\n",
    "            predicted_seq = predicted_seq.tolist()\n",
    "        best_number = predicted_seq.index(max(predicted_seq))\n",
    "        best_string = self.index_to_string[best_number]\n",
    "        best_token = self.string_to_token(best_string)\n",
    "        return [best_token]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "dataset = load_tokens(train_dir)\n",
    "code_completion = Code_Completion_Model(dataset)\n",
    "use_stored_model = False\n",
    "if use_stored_model:\n",
    "    code_completion.load_model(model_file)\n",
    "else:\n",
    "    train_time = code_completion.train()\n",
    "    code_completion.save_model(model_file)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('total time cost: %.2f s, model training cost: %.2f s'%(end_time-start_time, train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hole(tokens, max_hole_size = 2):\n",
    "    '''\n",
    "    input: a tokens sequence of source code and max_hole_size\n",
    "    return: hole token to be predicted (expection)\n",
    "            token sequence before the hole(prefix)\n",
    "            token sequence after the hole(suffix)\n",
    "    '''\n",
    "    hole_size = min(random.randint(1, max_hole_size), len(tokens) - 1)\n",
    "    hole_start_index = random.randint(1, len(tokens) - hole_size)\n",
    "    hole_end_index = hole_start_index + hole_size\n",
    "    prefix = tokens[0 : hole_start_index]\n",
    "    expection = tokens[hole_start_index : hole_end_index]\n",
    "    suffix = tokens[hole_end_index : 0]\n",
    "    return prefix, expection, suffix\n",
    "\n",
    "def token_equals(token1, token2):\n",
    "    '''\n",
    "    Determining whether input two tokens are equal or not\n",
    "    '''\n",
    "    if len(token1) != len(token2):\n",
    "        return False\n",
    "    for index, t1 in enumerate(token1):\n",
    "        t2 = token2[index]\n",
    "        if t1['type'] != t2['type'] or t1['value'] != t2['value']:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_test_data = load_tokens(query_dir)\n",
    "correct = 0\n",
    "correct_token_list = []\n",
    "incorrect_token_list = []\n",
    "for tokens in query_test_data:\n",
    "    prefix, expection, suffix = create_hole(tokens)\n",
    "    prediction = code_completion.query_test(prefix, suffix)\n",
    "    if token_equals(prediction, expection):\n",
    "        correct += 1\n",
    "        correct_token_list.append({'expection':expection, 'prediction':prediction})\n",
    "    else:\n",
    "        incorrect_token_list.append({'expection':expection, 'prediction':prediction})\n",
    "accuracy = correct / len(query_test_data)\n",
    "print('query test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('correct_token_list: \\n', correct_token_list[:5])\n",
    "print('incorrect_token_list: \\n', incorrect_token_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module\n",
    "optimization idea:\n",
    "- re-implement dnn model with tensorflow(not tflearn)\n",
    "- using embedding method rather thant one_hot_encoding\n",
    "- using a deeper and wider network\n",
    "- using LSTM\n",
    "- training model not with only one previous token, severl tokens? and following tokens?\n",
    "- try CNN\n",
    "- see each source code file as a training batch, do not combine them as a huge training data(for RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "\n",
    "\n",
    "import load_data\n",
    "\n",
    "\n",
    "train_dir = 'dataset/programs_800/'\n",
    "query_dir = 'dataset/programs_200/'\n",
    "model_dir = 'saved_model/sliding_window'\n",
    "\n",
    "\n",
    "slide_windows = [1,2]\n",
    "hidden_units = 128\n",
    "epoch_num = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "class Code_Completion_Model:\n",
    "    def __init__(self, token_lists):\n",
    "        '''\n",
    "        Initialize ML model with training data\n",
    "        token_lists: [[{type:.., value:..},{..},{..}], [..], [..]]\n",
    "        '''\n",
    "        self.token_lists = token_lists\n",
    "        self.tokens_set = set()\n",
    "        for token_sequence in token_lists:\n",
    "            for token in token_sequence:\n",
    "                self.tokens_set.add(self.token_to_string(token))\n",
    "        self.tokens_list = list(self.tokens_set)\n",
    "        self.tokens_list.sort()\n",
    "        self.tokens_num = len(self.tokens_set)  # 213\n",
    "        self.index_to_string = {i: s for i, s in enumerate(self.tokens_list)}\n",
    "        self.string_to_index = {s: i for i, s in enumerate(self.tokens_list)}\n",
    "\n",
    "    # data processing functions\n",
    "    def token_to_string(self, token):\n",
    "        return token['type'] + '~$$~' + token['value']\n",
    "\n",
    "    def string_to_token(self, string):\n",
    "        tokens = string.split('~$$~')\n",
    "        return {'type': tokens[0], 'value': tokens[1]}\n",
    "\n",
    "    # encoding token sequence as one_hot_encoding\n",
    "    def one_hot_encoding(self, string):\n",
    "        vector = [0] * self.tokens_num\n",
    "        vector[self.string_to_index[string]] = 1\n",
    "        return vector\n",
    "\n",
    "    # generate X_train data and y_label for ML model\n",
    "    def data_processing(self):\n",
    "        token_list = []\n",
    "        for token_sequence in self.token_lists:  # token_sequence of each source code\n",
    "            for index, token in enumerate(token_sequence):  # each token(type_value) in source code\n",
    "                token = self.token_to_string(token)\n",
    "                token_vec = self.one_hot_encoding(token)\n",
    "                token_list.append(token_vec)\n",
    "        return token_list\n",
    "\n",
    "    def split_with_windows(self, token_list, window_size):\n",
    "        #给定train_x, train_y list，元素由\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        for index, token_vec in enumerate(token_list):\n",
    "            if index > window_size - 1:\n",
    "                prev_token_list = []\n",
    "                for i in range(window_size):\n",
    "                    prev_token = token_list[index-i-1]\n",
    "                    prev_token_list.extend(prev_token)\n",
    "                train_x.append(prev_token_list)\n",
    "                train_y.append(token_vec)\n",
    "        return train_x, train_y\n",
    "    \n",
    "    # neural network functions\n",
    "    def create_NN(self, window_size):\n",
    "        tf.reset_default_graph()\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            input_x = tf.placeholder(tf.float32, [None, window_size * self.tokens_num],name='input_x')\n",
    "            output_y = tf.placeholder(tf.float32, [None, self.tokens_num],name='output_y')\n",
    "\n",
    "            fc1 = tf.layers.dense(\n",
    "                inputs=input_x, units=window_size*hidden_units,\n",
    "                activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(\n",
    "                inputs=fc1, units=window_size*hidden_units,\n",
    "                activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(\n",
    "                inputs=fc2, units=window_size * hidden_units,\n",
    "                activation=tf.nn.relu)\n",
    "            output_layer = tf.layers.dense(\n",
    "                inputs=fc3, units=self.tokens_num,activation=None,name='output_layer')\n",
    "\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=output_y)\n",
    "            loss = tf.reduce_mean(loss, name='loss')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, name='optimizer').minimize(loss)\n",
    "            accuracy = tf.equal(tf.argmax(output_layer, 1), tf.argmax(output_y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(accuracy, tf.float32), name='accuracy')\n",
    "        \n",
    "        return graph\n",
    "      #  return input_x, output_y, output_layer, loss, optimizer, accuracy\n",
    "\n",
    "\n",
    "\n",
    "        # load trained model into object\n",
    "    def load_model(self, model_file):\n",
    "        self.create_NN()\n",
    "        self.model.load(model_file)\n",
    "\n",
    "    # training ML model\n",
    "    def train(self):\n",
    "        print('model training...')\n",
    "        model_list = []\n",
    "\n",
    "        def get_batch(x_data, y_data):\n",
    "            for i in range(0, len(x_data), batch_size):\n",
    "                batch_x = x_data[i:i+batch_size]\n",
    "                batch_y = y_data[i:i+batch_size]\n",
    "                yield i//batch_size, batch_x, batch_y\n",
    "                \n",
    "        for window_size in slide_windows:\n",
    "            graph = self.create_NN(window_size) \n",
    "            #model = [input_x, output_y, output_layer, loss, optimizer, accuracy]\n",
    "            model_list.append(graph)\n",
    "        \n",
    "        token_list = self.data_processing()\n",
    "        for window_size, graph in zip(slide_windows, model_list):\n",
    "            print('with window_size: %d'%window_size)\n",
    "            x_data, y_data = self.split_with_windows(token_list, window_size)\n",
    "            with tf.Session(graph=graph) as sess:\n",
    "                saver = tf.train.Saver()\n",
    "                input_x = graph.get_tensor_by_name('input_x:0')\n",
    "                output_y = graph.get_tensor_by_name('output_y:0')\n",
    "                loss = graph.get_tensor_by_name('loss:0')\n",
    "                optimizer = graph.get_operation_by_name('optimizer')\n",
    "                accuracy = graph.get_tensor_by_name('accuracy:0')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                for epoch in range(epoch_num):\n",
    "                    geneator = get_batch(x_data, y_data)\n",
    "                    for i, batch_x, batch_y in geneator:\n",
    "                   #     batch_x, batch_y = next(geneator)\n",
    "                        feed = {input_x:batch_x, output_y:batch_y}\n",
    "                        sess.run(optimizer, feed)\n",
    "                        if(i%300 == 0):\n",
    "                            s_loss, s_accu = sess.run([loss, accuracy], feed)\n",
    "                            print('epoch: %d, step: %d, loss: %.2f, accuracy:%.2f'%\n",
    "                                  (epoch, i, s_loss, s_accu))\n",
    "                saver.save(sess, model_dir + '_' + str(window_size) + '.ckpt.meta')\n",
    "                    \n",
    "                \n",
    "\n",
    "\n",
    "    # query test\n",
    "    def query_test(self, prefix, suffix,window_size):\n",
    "        '''\n",
    "        Input: all tokens before the hole token(prefix) and all tokens after the hole token,\n",
    "        ML model will predict the most probable token in the hole\n",
    "        In this function, use only one token before hole token to predict\n",
    "        return: the most probable token\n",
    "        '''\n",
    "        test_tokens = []\n",
    "        new_saver = tf.train.import_meta_graph(model_dir + '.meta')\n",
    "        for i in range(window_size):\n",
    "            prev_token_string = self.token_to_string(prefix[-i-1])\n",
    "            x = self.one_hot_encoding(prev_token_string)\n",
    "            test_tokens.extend(x)\n",
    "        with tf.Session() as sess:\n",
    "            new_saver.restore(sess, model_dir)\n",
    "            graph = tf.get_default_graph()\n",
    "            input_x = graph.get_tensor_by_name('input_x:0')\n",
    "            accuracy = graph.get_tensor_by_name('accuracy:0')\n",
    "            output_layer = graph.get_tensor_by_name('output_layer:0')\n",
    "            feed = {input_x:test_tokens}\n",
    "            show_accu, prediction = sess.run([accuracy,output_layer], feed)\n",
    "            print('window_size: %d, accuracy:%.3f'%(window_size, show_accu))\n",
    "        return show_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data.load_tokens(query_dir, is_simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_model = Code_Completion_Model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_NN(self):\n",
    "    input_x = tf.placeholder([None, 1], dtype=tf.int32)\n",
    "    embedding_dim_size = 64\n",
    "    embedding_matrix = tf.Variable(tf.random_uniform([\n",
    "        self.tokens_size, embedding_dim_size], -1, 1), name='embedding_matrix')\n",
    "    input_embedding_layer = tf.nn.embedding_lookup(\n",
    "        embedding_matrix, input_x, name='embedding_layer')\n",
    "\n",
    "    def get_weight(shape):\n",
    "        initial = tf.random_uniform(shape)\n",
    "        return tf.Variable(initial, dtype=tf.float32)\n",
    "    def get_bias(shape):\n",
    "        initial = tf.constant(shape=shape, value=1.0)\n",
    "        return tf.Variable(initial, dtype=tf.float32)\n",
    "\n",
    "    w_fc1 = get_weight([embedding_dim_size, 1024])\n",
    "    b_fc1 = get_bias([1024])\n",
    "    z_fc1 = tf.matmul(w_fc1, embedding_matrix) + b_fc1\n",
    "    z_fc1 = tf.nn.relu(z_fc1)\n",
    "\n",
    "    w_fc2 = get_weight([1024, 1024])\n",
    "    b_fc2 = get_bias([1024])\n",
    "    z_fc2 = tf.matmul(w_fc2, z_fc1) + b_fc2\n",
    "    z_fc2 = tf.nn.relu(z_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN(self):\n",
    "    x = tf.placeholder(tf.float32, [None, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seee = set((1,2,3,4))\n",
    "for i, s in enumerate(seee):\n",
    "    print(i, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
