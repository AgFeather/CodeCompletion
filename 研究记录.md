# Code Completion System

## Dataset
### Dataset Description
[dataset](https://www.sri.inf.ethz.ch/js150)
#### train dataset
1. contains 100,000 JavaScript source code which has been parsed into AST format.
2. size of dataset: 11 GB
#### test dataset
1. contains 50,000 JavaScript source code which has been parsed into AST format.
2. size of dataset: 4.8 GB
#### AST structure
Each line of dataset is a list which means a js code file, the elements of a list is a dictionary, contains keys below.
- (Required) id: unique integer identifying current AST node.
- (Required) type: string containing type of current AST node.
- (Optional) value: string containing value (if any) of the current AST node.
- (Optional) children: array of integers denoting children (if any) of the current AST node.
#### example
```
console.log("Hello World!");

[ { "id":0, "type":"Program", "children":[1] },
    { "id":1, "type":"ExpressionStatement", "children":[2] },
      { "id":2, "type":"CallExpression", "children":[3,6] },
        { "id":3, "type":"MemberExpression", "children":[4,5] },
          { "id":4, "type":"Identifier", "value":"console" },
          { "id":5, "type":"Property", "value":"log" },
        { "id":6, "type":"LiteralString", "value":"Hello World!" }, 0]
```


### LSTM model 2018-11-20
#### note:
1. without validation
2. 100,000 AST for train; 50,000 AST for test
#### data processing
1. there are 6,970,900 nt_pair in training dataset
2. there are 4,706,813 nt_pair in test dataset,(4,076,500 long seq + 30,389 short seq)
#### hyper-paremeter
1. batch_size: 64
2. number of lstm layers: 2
3. number of hidden units: 256
4. learning rate: 0.001 without rate decky
5. number of epoches: 20
6. non-terminal token embed dimension: 64
7. terminal token embed dimension: 200
8. time steps: 50




### LSTM model 2018-11-20
#### change
1. 将test data的前5,000 * 6 = 30,000个AST放入到training dataset中，第7，8个AST subset单独拿出来作为validation dataset，保留第9，10个AST subset作为test dataset。
### 测试数据集
1. 在测试数据集中，一共用了50,000个AST文件，生成non-terminal terminal sequence后，在不进行任何截取的情况下一共有4,706,813个nt_pair。
2. 在将所有长度小于time_steps（==50）的nt_seq舍去的情况下，还剩余4,076,500个nt_pair

#### Origin LSTM model 2018-12-11
模型完全争取按照paper实现，删除了placeholder的大小限制，进而争取在测试阶段可以一次性输入多个time-step的token。
在训练和验证集上的准确率都很高，但在测试集上准确率只有50%左右

#### Origin LSTM model 2018-12-13
在原来的模型基础上，实现了下一次训练batch是根据上一次的final state来进行训练的可能
在训练测试和验证集上的准确率都达到了84%左右，基本实现了paper上的准确率
**目前为止最好的模型**
##### 问题
因为在测试过程中采用了time-step上的批输入，如果输入的time-step过于长，会导致GPU内存爆炸。

#### GRU 2018-12-14
在其他参数都相同的情况下，换成GRU model。对比lstm，GRU的训练快了0.02s。
注意到时候比较训练时模型的收敛速度和对应的测试、验证准确率
##### 结论
1. GRU对比lstm，训练速度快了0.05s per batch。每个epoch快了300s
2. 从loss和训练准确率曲线来看，GRUloss值更小，两个准确率都稍微更高一些
3. GRU和LSTM的验证准确率差不多
3. 但在测试过程，GRU在non-terminal上的预测准确率反倒低于LSTM

#### Multi_LSTM 2018-12-15
使用2层的LSTM进行训练
##### 结论

#### paper LSTM with new data processing 2018-12-18
在构建word2vec时，由于存在一个error导致对整个data processing模块进行检查，查出了一个大的bug，之前的data processing丢掉了非常多的ast data。
修改后data processing可以生成的数据量达到了原来数据量的10倍，不得已重新在这个数据量上运行paper模型。

**注意：有一点，忘了修改learning rate的decay速率，导致模型到后期训练不动了**

#### paper LSTM with new data processing 2018-12-20
修改了18号模型，learning rate过低的错误，准确率得到了提升

#### LSTM with negative sample 2018-12-22
在paper lstm模型的基础上，对terminal的预测增加了 **负采样方法**，模型的训练速度得到提高，但训练准确率有点下降

#### Double LSTM model 2018-12-25
分别使用两个LSTM去预测

#### 2018-12-29 模型优化
1. 将tf.nn.softmax_cross_entropy_with_logits_v2 损失改为tf.nn.sparse_softmax_cross_entropy_with_logits损失。
2. 在计算准确率：build_accuracy中，将one_hot_target改为了int值。
通过以上两个方法， 不在需要将target label转换为one-hot-encoding。大大加速模型训练速度。（加速比10%）
**该模型运行了5个epochs，但测试上的预测准确率变低了**

#### 2018-12-31 模型优化
实现了topk预测和topk训练准确率显示，同时训练5个epoch，到时候分别测试各个epoch生成的模型的准确率。查找最优epoch数。
在训练时不计算softmax，减少内存资源占用率，加快运行速度

#### 2019-01-07 LSTM 运行10个epoch
运行10个epoch，然后测试每个epoch的准确率，找到最佳epoch，减少模型训练时间。

## TODO
### 使用多层lstm
### attention机制
attention相当于优化了long-dependencies
注意：使用attention模型会变的更大，据说训练耗时也要增加很多。
Attention可以可视化，寻找可视化方法并实现
**直接在paper lstm基础上增加Attention并不理想，模型变得非常大，难以训练**
试着在word2vec减少了embedding dim的基础上构建一个attention
### 实现一个实时的小例子
实现一个：读入JS source code；转换为AST；转换为NT-pair；进行预测的例子
### 使用word2vec
#### 问题
##### 原始数据为一个个nt-pair，包含两个token，如何将其转换为一个token？
1. 最直接的方法，直接将nt-pair的括号拆掉，变成两个独立的token在一个句子中。
2. 重新尽心in-order-traversal的遍历，得到一个sequence of token








## Code Completion System
### 研究想法
- 构建两个神经网络，一个负责提取AST的结构，一个负责在对time-sequence数据进行特征提取，然后将两者拼接到一起，进行最后的预测。
- 构建一个LSTM-CNN 神经网络，将LSTM的每一步的输出作为CNN的输入，然后CNN的输出预测下一个token
#### Tree_based LSTM
Sentences are not a simple linear sequence
##### models
###### child-sum tree LSTM
sums over all the children of a node: can be used for any number of children
###### N-ary tree LSTM
Use different parameters for each node: better granularity, but maximum number of children per node must be fixed.

### 问题
1. 过多的unknown token 导致对terminal的预测准确率过高？
2. 数据处理时，统计unknown token的个数，出现个数，以及known token在所有token的占比问题。看论文中是如何处理这个问题的。
4. 重新看论文中模型的各个细节，提高准确率
5. 测试过程为每次仅输入一个token，效率过低，是否有方法提高？





为什么将non-terminal和terminal分开：防止non-terminal过拟合。分开后可以分别构建两个loss，并平衡两个loss的权重防止对non-terminal的更新过大，导致过拟合。

现在在个训练step的时候已经不在是init_zero_state了，而是用上一时刻的final output state作为输出，那么问题又变成了：如果继续用上次的输出，batch的转换就存在了问题（之前的做法相当于将每个sequence掰开，掰成多个batch并行训练，也就是说按照这种batch转换方式，对于一个sequence是没办法用到上一时刻的state信息的），实话说，原始处理方法简单粗暴，不同于paper中的描述，所以需要解决如何对非常长的sequence进行保留信息的处理问题。



### notes
1. loss曲线的抖动可以理解为是因为采用了mini-batch，而不是对所有数据进行计算更新。



## Node2Vec
将AST中每个terminal Node都进行embedding，思想类似于Word2Vec，性质相同的Node具有相同的上下文环境，所以对于每个terminal node构建一个训练对(x, n_y, t_y), x为要训练的terminal node，n_y为其对应的non-teminal父节点，t_y为一个list，对应着x所有的terminal兄弟节点。
### 考虑
1. n_y是否也可以是一个list？如果表示non-terminal父节点，non-terminal父父节点等
2. 大胆的想法：只用AST这个结构训练各个token的embedding，然后用sequence of token在序列上用LSTM进行预测，这样的话可以快速的实现一个runtime的code completion system。
3. 加入同一个non-terminal节点下要训练的terminal node的Order信息？因为如果不加入Order信息的话，各个terminal的信息表示除了该node本身就是相同的了
4. 可以设置两个参数max_depth和max_width，分别表示non-terminal父节点的个数和terminal兄弟节点的个数用来表示上下文信息。
