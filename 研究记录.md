# Code Completion System

## Dataset
### Dataset Description
[dataset](https://www.sri.inf.ethz.ch/js150)
#### train dataset
1. contains 100,000 JavaScript source code which has been parsed into AST format.
2. size of dataset: 11 GB
#### test dataset
1. contains 50,000 JavaScript source code which has been parsed into AST format.
2. size of dataset: 4.8 GB
#### AST structure
Each line of dataset is a list which means a js code file, the elements of a list is a dictionary, contains keys below.
- (Required) id: unique integer identifying current AST node.
- (Required) type: string containing type of current AST node.
- (Optional) value: string containing value (if any) of the current AST node.
- (Optional) children: array of integers denoting children (if any) of the current AST node.
#### example
```
console.log("Hello World!");

[ { "id":0, "type":"Program", "children":[1] },
    { "id":1, "type":"ExpressionStatement", "children":[2] },
      { "id":2, "type":"CallExpression", "children":[3,6] },
        { "id":3, "type":"MemberExpression", "children":[4,5] },
          { "id":4, "type":"Identifier", "value":"console" },
          { "id":5, "type":"Property", "value":"log" },
        { "id":6, "type":"LiteralString", "value":"Hello World!" }, 0]
```


### LSTM model 2018-11-20
#### note:
1. without validation
2. 100,000 AST for train; 50,000 AST for test
#### data processing
1. there are 6,970,900 nt_pair in training dataset
2. there are 4,706,813 nt_pair in test dataset,(4,076,500 long seq + 30,389 short seq)
#### hyper-paremeter
1. batch_size: 64
2. number of lstm layers: 2
3. number of hidden units: 256
4. learning rate: 0.001 without rate decky
5. number of epoches: 20
6. non-terminal token embed dimension: 64
7. terminal token embed dimension: 200
8. time steps: 50




### LSTM model 2018-11-20
#### change
1. 将test data的前5,000 * 6 = 30,000个AST放入到training dataset中，第7，8个AST subset单独拿出来作为validation dataset，保留第9，10个AST subset作为test dataset。
### 测试数据集
1. 在测试数据集中，一共用了50,000个AST文件，生成non-terminal terminal sequence后，在不进行任何截取的情况下一共有4,706,813个nt_pair。
2. 在将所有长度小于time_steps（==50）的nt_seq舍去的情况下，还剩余4,076,500个nt_pair

#### Origin LSTM model 2018-12-11
模型完全争取按照paper实现，删除了placeholder的大小限制，进而争取在测试阶段可以一次性输入多个time-step的token。
在训练和验证集上的准确率都很高，但在测试集上准确率只有50%左右

#### Origin LSTM model 2018-12-13
在原来的模型基础上，实现了下一次训练batch是根据上一次的final state来进行训练的可能
在训练测试和验证集上的准确率都达到了84%左右，基本实现了paper上的准确率
**目前为止最好的模型**
##### 问题
因为在测试过程中采用了time-step上的批输入，如果输入的time-step过于长，会导致GPU内存爆炸。

#### GRU 2018-12-14
在其他参数都相同的情况下，换成GRU model。对比lstm，GRU的训练快了0.02s。
注意到时候比较训练时模型的收敛速度和对应的测试、验证准确率
##### 结论
1. GRU对比lstm，训练速度快了0.05s per batch。每个epoch快了300s
2. 从loss和训练准确率曲线来看，GRUloss值更小，两个准确率都稍微更高一些
3. GRU和LSTM的验证准确率差不多
3. 但在测试过程，GRU在non-terminal上的预测准确率反倒低于LSTM

#### Multi_LSTM 2018-12-15
使用2层的LSTM进行训练
##### 结论

#### paper LSTM with new data processing 2018-12-18
在构建word2vec时，由于存在一个error导致对整个data processing模块进行检查，查出了一个大的bug，之前的data processing丢掉了非常多的ast data。
修改后data processing可以生成的数据量达到了原来数据量的10倍，不得已重新在这个数据量上运行paper模型。

**注意：有一点，忘了修改learning rate的decay速率，导致模型到后期训练不动了**

#### paper LSTM with new data processing 2018-12-20
修改了18号模型，learning rate过低的错误，准确率得到了提升

#### LSTM with negative sample 2018-12-22
在paper lstm模型的基础上，对terminal的预测增加了 **负采样方法**，模型的训练速度得到提高，但训练准确率有点下降

#### Double LSTM model 2018-12-25
分别使用两个LSTM去预测

#### 2018-12-29 模型优化
1. 将tf.nn.softmax_cross_entropy_with_logits_v2 损失改为tf.nn.sparse_softmax_cross_entropy_with_logits损失。
2. 在计算准确率：build_accuracy中，将one_hot_target改为了int值。
通过以上两个方法， 不在需要将target label转换为one-hot-encoding。大大加速模型训练速度。（加速比10%）
**该模型运行了5个epochs，但测试上的预测准确率变低了**

#### 2018-12-31 模型优化
实现了topk预测和topk训练准确率显示，同时训练5个epoch，到时候分别测试各个epoch生成的模型的准确率。查找最优epoch数。
在训练时不计算softmax，减少内存资源占用率，加快运行速度

#### 2019-01-07 LSTM 运行10个epoch
运行10个epoch，然后测试每个epoch的准确率，找到最佳epoch，减少模型训练时间。

## TODO
### 使用多层lstm
### attention机制
attention相当于优化了long-dependencies
注意：使用attention模型会变的更大，据说训练耗时也要增加很多。
Attention可以可视化，寻找可视化方法并实现
**直接在paper lstm基础上增加Attention并不理想，模型变得非常大，难以训练**
试着在word2vec减少了embedding dim的基础上构建一个attention
### 实现一个实时的小例子
实现一个：读入JS source code；转换为AST；转换为NT-pair；进行预测的例子
### 使用word2vec
#### 问题
##### 原始数据为一个个nt-pair，包含两个token，如何将其转换为一个token？
1. 最直接的方法，直接将nt-pair的括号拆掉，变成两个独立的token在一个句子中。
2. 重新尽心in-order-traversal的遍历，得到一个sequence of token








## Code Completion System
### 研究想法
- 构建两个神经网络，一个负责提取AST的结构，一个负责在对time-sequence数据进行特征提取，然后将两者拼接到一起，进行最后的预测。
- 构建一个LSTM-CNN 神经网络，将LSTM的每一步的输出作为CNN的输入，然后CNN的输出预测下一个token
#### Tree_based LSTM
Sentences are not a simple linear sequence
##### models
###### child-sum tree LSTM
sums over all the children of a node: can be used for any number of children
###### N-ary tree LSTM
Use different parameters for each node: better granularity, but maximum number of children per node must be fixed.

### 问题
1. 过多的unknown token 导致对terminal的预测准确率过高？
2. 数据处理时，统计unknown token的个数，出现个数，以及known token在所有token的占比问题。看论文中是如何处理这个问题的。
4. 重新看论文中模型的各个细节，提高准确率
5. 测试过程为每次仅输入一个token，效率过低，是否有方法提高？





为什么将non-terminal和terminal分开：防止non-terminal过拟合。分开后可以分别构建两个loss，并平衡两个loss的权重防止对non-terminal的更新过大，导致过拟合。

现在在个训练step的时候已经不在是init_zero_state了，而是用上一时刻的final output state作为输出，那么问题又变成了：如果继续用上次的输出，batch的转换就存在了问题（之前的做法相当于将每个sequence掰开，掰成多个batch并行训练，也就是说按照这种batch转换方式，对于一个sequence是没办法用到上一时刻的state信息的），实话说，原始处理方法简单粗暴，不同于paper中的描述，所以需要解决如何对非常长的sequence进行保留信息的处理问题。



### notes
1. loss曲线的抖动可以理解为是因为采用了mini-batch，而不是对所有数据进行计算更新。



## Node2Vec
将AST中每个terminal Node都进行embedding，思想类似于Word2Vec，性质相同的Node具有相同的上下文环境，所以对于每个terminal node构建一个训练对(x, n_y, t_y), x为要训练的terminal node，n_y为其对应的non-teminal父节点，t_y为一个list，对应着x所有的terminal兄弟节点。
### 考虑
1. n_y是否也可以是一个list？如果表示non-terminal父节点，non-terminal父父节点等
2. 大胆的想法：只用AST这个结构训练各个token的embedding，然后用sequence of token在序列上用LSTM进行预测，这样的话可以快速的实现一个runtime的code completion system。
3. 加入同一个non-terminal节点下要训练的terminal node的Order信息？因为如果不加入Order信息的话，各个terminal的信息表示除了该node本身就是相同的了
4. 可以设置两个参数max_depth和max_width，分别表示non-terminal父节点的个数和terminal兄弟节点的个数用来表示上下文信息。




### research idea
1. 一个更好的evaluation method？比如说，原paper中是随机创建hole的，但在刚刚开始写程序时，code completion可能不会产生作用，所以可以试着根据真实应用情况，refine a new accuracy calculation better.
2. 给每个node添加更多的信息，比如说是否有right child等，进而可以从sequence重新构建一个新的AST。（目前看很难完成）
3. 构建一个语法检查器，进而在LSTM完成预测后，对预测结果进行语法检查，如果不符合语法规则，则直接放弃该预测，用top2。（需要定义在给定AST和给定seq下什么语法是illegal的，什么是合法的，比较难）
4. 给每个variable更多的信息，也就是variable是哪类的variable。这样相当于重新构建了整个数据处理阶段，而且预测也更加有意义（但是JS语言的variable记录问题？）。具体操作方法是，遍历整个AST，并找到每个definition，记录每个variable是什么类型的，然后在最后生成training pair的时候，将整个variable的type也加入到encoding中。（会产生更多种类的non-terminal和terminal）
5. rewrite每个变量的名字，改成arg1，arg2.这样就大大减少了token的种类，并在预测中对下一个token会是之前出现的哪种arg进行预测
6. 不仅仅是non-terminal，terminal pair，可以变成一个truple pair，多加一个信息，比如说terminal的自然语言含义之类的信息。
7. 对于人类，可以理解length，maxNumber等变量的字面意义，比如说在定义循环条件的时候，大于号小于号后面就很可能出现length，这样的自然语义，LSTM是不会理解的，试着加进去？或者用什么方法对其进行一个预训练？（Word2Vec之类的）

8. 统计有多少种token是out of bag的，然后统计经过增加变量type，改写variable名字之后，有多少token是out of bag的。（猜想应该可以减少out of bag的数量，但同时又增加了更多的信息）

9. 新的accuracy：仅仅计算对variable的预测准确率，相信改写成arg1，arg2 的格式后，这个准确率会有一个提升



## doing
1. focus on 'identifier prediction'
    1. rename identifi er as 'arg1', 'arg2' in each file.
    2. Design a new evaluation method like 'identifier prediction', because

2. Node2Vec embedding representation method
    1. like word2vec, it can make DL model smaller
    2. with smaller model, both training step and eval step may cost less time
    3. because each embedding representation vector is trained by token's context, with Node2Vec, performance of prediction may be better. (I guess)

## worth to try but hard
1. Name of variable is a useful information, it may contain some 'natural language information' like 'length', 'maxNumber'.etc.
Idea is try to use this information. (use word2vec to get it's natural information representation and create not only (non-terminal, terminal) pair, but also (non-terminal, terminal, natural-language-info) truple)
